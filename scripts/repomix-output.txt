This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-11-04T09:00:52.853Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
utils/
  __init__.py
  data_utils.py
  eeve_adapter.py
  eeve_initialization.py
  model_utils.py
  train_utils.py
base_trainer.py
evaluate_models.py
stage1.py
stage2.py
stage3.py
stage4.py
stage5.py
stage6.py

================================================================
Files
================================================================

================
File: utils/__init__.py
================
"""
Common utility modules for Korean Embedding Expansion
"""

from .model_utils import *
from .data_utils import *
from .train_utils import *
from .eeve_adapter import *

__all__ = [
    # model_utils
    'load_tokenizers',
    'load_model',
    'freeze_model_params',
    'expand_embeddings',
    'save_model',

    # data_utils
    'load_dataset',
    'create_dataloader',
    'prepare_batch',
    'TextDataset',
    'PairDataset',

    # train_utils
    'setup_distributed',
    'setup_optimizer',
    'setup_scheduler',
    'simcse_loss',
    'contrastive_loss',
    'mean_pooling',
    'is_main_process',
    'barrier',
    'save_checkpoint',
    'load_checkpoint',

    # eeve_adapter
    'AdapterLayer',
    'GatedAdapter',
    'ParallelAdapter',
    'inject_adapters',
    'freeze_except_adapters',
    'get_adapter_params',
]

================
File: utils/data_utils.py
================
"""
Data utilities for loading and processing datasets
"""

import logging
from typing import Dict, List, Optional
from pathlib import Path

import torch
from torch.utils.data import Dataset, DataLoader, DistributedSampler
from datasets import load_dataset as hf_load_dataset
from tqdm import tqdm

logger = logging.getLogger(__name__)


class TextDataset(Dataset):
    """
    í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ (SimCSE ë¬´ì§€ë„ í•™ìŠµìš©)
    """

    def __init__(
        self,
        texts: List[str],
        tokenizer,
        max_length: int = 128
    ):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        text = self.texts[idx]

        # í† í¬ë‚˜ì´ì§•
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0)
        }


class PairDataset(Dataset):
    """
    Pair ë°ì´í„°ì…‹ (ê°ë… í•™ìŠµìš©)
    """

    def __init__(
        self,
        pairs: List[Dict],
        tokenizer,
        max_length: int = 256
    ):
        """
        Args:
            pairs: [{"text1": "...", "text2": "...", "label": 0/1}, ...]
            tokenizer: í† í¬ë‚˜ì´ì €
            max_length: ìµœëŒ€ ê¸¸ì´
        """
        self.pairs = pairs
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        pair = self.pairs[idx]

        # Text 1
        enc1 = self.tokenizer(
            pair['text1'],
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        # Text 2
        enc2 = self.tokenizer(
            pair['text2'],
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'input_ids_1': enc1['input_ids'].squeeze(0),
            'attention_mask_1': enc1['attention_mask'].squeeze(0),
            'input_ids_2': enc2['input_ids'].squeeze(0),
            'attention_mask_2': enc2['attention_mask'].squeeze(0),
            'label': torch.tensor(pair.get('label', 1), dtype=torch.long)
        }


def load_dataset(
    dataset_name: str,
    text_field: str = "text",
    split: str = "train",
    max_samples: Optional[int] = None,
    shuffle: bool = True,
    streaming: bool = False,
    cache_dir: Optional[str] = None
) -> List[str]:
    """
    HuggingFace ë°ì´í„°ì…‹ ë¡œë“œ

    Args:
        dataset_name: ë°ì´í„°ì…‹ ì´ë¦„
        text_field: í…ìŠ¤íŠ¸ í•„ë“œ ì´ë¦„
        split: ìŠ¤í”Œë¦¿ ("train", "validation", "test")
        max_samples: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜
        shuffle: ì…”í”Œ ì—¬ë¶€
        streaming: ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
        cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬

    Returns:
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    logger.info("=" * 80)
    logger.info(f"ë°ì´í„°ì…‹ ë¡œë”©: {dataset_name}")
    logger.info("=" * 80)
    logger.info(f"Split: {split}")
    logger.info(f"Max samples: {max_samples if max_samples else 'All'}")
    logger.info(f"Streaming: {streaming}")

    try:
        # ë°ì´í„°ì…‹ ë¡œë“œ
        dataset = hf_load_dataset(
            dataset_name,
            split=split,
            streaming=streaming,
            cache_dir=cache_dir
        )

        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        texts = []

        if streaming:
            # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
            for i, example in enumerate(dataset):
                if max_samples and i >= max_samples:
                    break
                if text_field in example:
                    texts.append(example[text_field])
        else:
            # ì¼ë°˜ ëª¨ë“œ
            if max_samples:
                dataset = dataset.select(range(min(max_samples, len(dataset))))

            if shuffle:
                dataset = dataset.shuffle(seed=42)

            texts = [ex[text_field] for ex in tqdm(dataset, desc="í…ìŠ¤íŠ¸ ì¶”ì¶œ")]

        # í•„í„°ë§ (ë¹ˆ í…ìŠ¤íŠ¸ ì œê±°)
        texts = [t for t in texts if t and len(t.strip()) > 0]

        logger.info(f"âœ“ ë¡œë“œ ì™„ë£Œ: {len(texts):,}ê°œ ìƒ˜í”Œ")
        logger.info("")

        return texts

    except Exception as e:
        logger.error(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise


def create_dataloader(
    dataset: Dataset,
    batch_size: int,
    num_workers: int = 4,
    shuffle: bool = True,
    pin_memory: bool = True,
    distributed: bool = False,
    world_size: int = 1,
    rank: int = 0
) -> DataLoader:
    """
    DataLoader ìƒì„±

    Args:
        dataset: ë°ì´í„°ì…‹
        batch_size: ë°°ì¹˜ í¬ê¸°
        num_workers: Worker ìˆ˜
        shuffle: ì…”í”Œ ì—¬ë¶€
        pin_memory: Pin memory ì‚¬ìš© ì—¬ë¶€
        distributed: ë¶„ì‚° í•™ìŠµ ì—¬ë¶€
        world_size: ì „ì²´ í”„ë¡œì„¸ìŠ¤ ìˆ˜
        rank: í˜„ìž¬ í”„ë¡œì„¸ìŠ¤ rank

    Returns:
        DataLoader
    """
    sampler = None

    if distributed:
        sampler = DistributedSampler(
            dataset,
            num_replicas=world_size,
            rank=rank,
            shuffle=shuffle
        )
        shuffle = False  # samplerê°€ ìžˆìœ¼ë©´ shuffleì€ False

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        sampler=sampler,
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=True  # ë§ˆì§€ë§‰ ë¶ˆì™„ì „í•œ ë°°ì¹˜ ì œê±°
    )

    return dataloader


def prepare_batch(batch: Dict, device: torch.device) -> Dict:
    """
    ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™

    Args:
        batch: ë°°ì¹˜ ë”•ì…”ë„ˆë¦¬
        device: íƒ€ê²Ÿ ë””ë°”ì´ìŠ¤

    Returns:
        ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ëœ ë°°ì¹˜
    """
    return {k: v.to(device) if isinstance(v, torch.Tensor) else v
            for k, v in batch.items()}

================
File: utils/eeve_adapter.py
================
"""
EEVE-style Adapter Modules
ì–‡ì€ ì¶”ê°€ ë ˆì´ì–´ë¥¼ Transformer ë¸”ë¡ì— ì‚½ìž…í•˜ì—¬ í•œêµ­ì–´ ì ì‘
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional


class AdapterLayer(nn.Module):
    """
    EEVE-style Adapter Layer

    Bottleneck êµ¬ì¡°:
    hidden_size -> down_size -> activation -> up_size -> hidden_size
    """

    def __init__(
        self,
        hidden_size: int,
        adapter_size: int = 256,
        dropout: float = 0.1,
        init_scale: float = 0.01
    ):
        super().__init__()

        self.hidden_size = hidden_size
        self.adapter_size = adapter_size

        # Down projection
        self.down_proj = nn.Linear(hidden_size, adapter_size, bias=True)

        # Up projection
        self.up_proj = nn.Linear(adapter_size, hidden_size, bias=True)

        # Activation
        self.activation = nn.GELU()

        # Dropout
        self.dropout = nn.Dropout(dropout)

        # Gate (learnable scaling)
        self.gate = nn.Parameter(torch.zeros(1))

        # ìž‘ì€ ê°’ìœ¼ë¡œ ì´ˆê¸°í™” (ì•ˆì •ì„±)
        nn.init.normal_(self.down_proj.weight, std=init_scale)
        nn.init.zeros_(self.down_proj.bias)
        nn.init.normal_(self.up_proj.weight, std=init_scale)
        nn.init.zeros_(self.up_proj.bias)

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        """
        Args:
            hidden_states: [batch_size, seq_len, hidden_size]

        Returns:
            adapted_hidden_states: [batch_size, seq_len, hidden_size]
        """
        residual = hidden_states

        # Adapter forward
        x = self.down_proj(hidden_states)
        x = self.activation(x)
        x = self.dropout(x)
        x = self.up_proj(x)

        # Gated residual connection
        output = residual + self.gate * x

        return output


class GatedAdapter(nn.Module):
    """
    Gated Adapter with separate gates for different pathways
    """

    def __init__(
        self,
        hidden_size: int,
        adapter_size: int = 256,
        dropout: float = 0.1,
        init_scale: float = 0.01
    ):
        super().__init__()

        # Adapter
        self.down_proj = nn.Linear(hidden_size, adapter_size, bias=True)
        self.up_proj = nn.Linear(adapter_size, hidden_size, bias=True)
        self.activation = nn.GELU()
        self.dropout = nn.Dropout(dropout)

        # Gating network
        self.gate_linear = nn.Linear(hidden_size, 1, bias=True)

        # ì´ˆê¸°í™”
        nn.init.normal_(self.down_proj.weight, std=init_scale)
        nn.init.zeros_(self.down_proj.bias)
        nn.init.normal_(self.up_proj.weight, std=init_scale)
        nn.init.zeros_(self.up_proj.bias)
        nn.init.zeros_(self.gate_linear.weight)
        nn.init.constant_(self.gate_linear.bias, -3.0)  # ì´ˆê¸°ì—ëŠ” ê±°ì˜ ë‹«íž˜

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        residual = hidden_states

        # Adapter forward
        x = self.down_proj(hidden_states)
        x = self.activation(x)
        x = self.dropout(x)
        x = self.up_proj(x)

        # Dynamic gating
        gate = torch.sigmoid(self.gate_linear(hidden_states))

        # Gated combination
        output = residual + gate * x

        return output


class ParallelAdapter(nn.Module):
    """
    Parallel Adapter (ë³‘ë ¬ êµ¬ì¡°)
    ì›ë³¸ ë¸”ë¡ê³¼ ì–´ëŒ‘í„°ë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰ í›„ ê²°í•©
    """

    def __init__(
        self,
        hidden_size: int,
        adapter_size: int = 256,
        dropout: float = 0.1,
        init_scale: float = 0.01
    ):
        super().__init__()

        # Adapter branch
        self.adapter = nn.Sequential(
            nn.Linear(hidden_size, adapter_size, bias=True),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(adapter_size, hidden_size, bias=True)
        )

        # Mixing weight
        self.alpha = nn.Parameter(torch.tensor(0.1))

        # ì´ˆê¸°í™”
        for module in self.adapter:
            if isinstance(module, nn.Linear):
                nn.init.normal_(module.weight, std=init_scale)
                nn.init.zeros_(module.bias)

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        # Parallel paths
        adapter_output = self.adapter(hidden_states)

        # Weighted combination
        output = hidden_states + self.alpha * adapter_output

        return output


class HierarchicalAdapter(nn.Module):
    """
    Hierarchical Adapter (Phase Bìš© ë‹¨ìˆœ ë²„ì „)

    2-layer êµ¬ì¡°:
    - Layer 1: Language-specific (Korean focused)
    - Layer 2: Task-specific (embedding)

    Full version (cross-lingual, dynamic routing ë“±)ì€ ë‚˜ì¤‘ì—
    """

    def __init__(
        self,
        hidden_size: int,
        adapter_size: int = 256,
        dropout: float = 0.1,
        init_scale: float = 0.01
    ):
        super().__init__()

        # Layer 1: Language-specific adapter (Korean)
        self.language_adapter = nn.Sequential(
            nn.Linear(hidden_size, adapter_size, bias=True),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(adapter_size, hidden_size, bias=True)  # ìˆ˜ì •: hidden_size ì¶œë ¥
        )

        # Layer 2: Task-specific adapter (Embedding)
        self.task_adapter = nn.Sequential(
            nn.Linear(hidden_size, adapter_size // 2, bias=True),  # ìˆ˜ì •: hidden_size ìž…ë ¥
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(adapter_size // 2, hidden_size, bias=True)
        )

        # Learnable gates
        self.gate1 = nn.Parameter(torch.zeros(1))
        self.gate2 = nn.Parameter(torch.zeros(1))

        # ì´ˆê¸°í™”
        for module in [self.language_adapter, self.task_adapter]:
            for layer in module:
                if isinstance(layer, nn.Linear):
                    nn.init.normal_(layer.weight, std=init_scale)
                    nn.init.zeros_(layer.bias)

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        """
        2-layer hierarchical forward

        Args:
            hidden_states: [B, L, hidden_size]

        Returns:
            output: [B, L, hidden_size]
        """
        residual = hidden_states

        # Layer 1: Language-specific (ì´ì œ ì°¨ì›ì´ ë§žìŒ)
        lang_output = self.language_adapter(hidden_states)
        lang_output = hidden_states + self.gate1 * lang_output

        # Layer 2: Task-specific
        task_output = self.task_adapter(lang_output)
        output = residual + self.gate2 * task_output

        return output


def inject_adapters(
    model: nn.Module,
    adapter_type: str = "bottleneck",
    adapter_size: int = 256,
    dropout: float = 0.1,
    layer_indices: Optional[list] = None
) -> nn.Module:
    """
    Transformer ëª¨ë¸ì— ì–´ëŒ‘í„° ì£¼ìž…

    Args:
        model: Transformer ëª¨ë¸
        adapter_type: "bottleneck", "gated", "parallel"
        adapter_size: ì–´ëŒ‘í„° hidden size
        dropout: Dropout rate
        layer_indices: ì–´ëŒ‘í„°ë¥¼ ì¶”ê°€í•  ë ˆì´ì–´ ì¸ë±ìŠ¤ (Noneì´ë©´ ì „ì²´)

    Returns:
        ì–´ëŒ‘í„°ê°€ ì¶”ê°€ëœ ëª¨ë¸
    """
    # Qwen ëª¨ë¸ êµ¬ì¡° í™•ì¸
    if hasattr(model, 'model') and hasattr(model.model, 'layers'):
        layers = model.model.layers
    elif hasattr(model, 'layers'):
        layers = model.layers
    else:
        raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ êµ¬ì¡°ìž…ë‹ˆë‹¤.")

    # ì–´ëŒ‘í„° íƒ€ìž… ì„ íƒ
    adapter_class = {
        "bottleneck": AdapterLayer,
        "gated": GatedAdapter,
        "parallel": ParallelAdapter,
        "hierarchical": HierarchicalAdapter
    }.get(adapter_type, AdapterLayer)

    # ë ˆì´ì–´ ì¸ë±ìŠ¤ ê²°ì •
    if layer_indices is None:
        layer_indices = list(range(len(layers)))

    # ê° ë ˆì´ì–´ì— ì–´ëŒ‘í„° ì¶”ê°€
    for idx in layer_indices:
        if idx >= len(layers):
            continue

        layer = layers[idx]
        # Qwen3/Qwen2ëŠ” config.hidden_size ì‚¬ìš©
        if hasattr(model, 'config') and hasattr(model.config, 'hidden_size'):
            hidden_size = model.config.hidden_size
        elif hasattr(layer, 'self_attn') and hasattr(layer.self_attn, 'embed_dim'):
            hidden_size = layer.self_attn.embed_dim
        else:
            hidden_size = 768  # default fallback

        # ì–´ëŒ‘í„° ìƒì„±
        adapter = adapter_class(
            hidden_size=hidden_size,
            adapter_size=adapter_size,
            dropout=dropout
        )

        # ë ˆì´ì–´ì˜ dtypeê³¼ deviceë¥¼ ê°ì§€í•˜ì—¬ ì–´ëŒ‘í„°ë¥¼ ê°™ì€ dtype/deviceë¡œ ì´ë™
        # ì²« ë²ˆì§¸ íŒŒë¼ë¯¸í„°ì˜ dtypeê³¼ deviceë¥¼ ì‚¬ìš©
        first_param = next(layer.parameters())
        adapter = adapter.to(dtype=first_param.dtype, device=first_param.device)

        # ë ˆì´ì–´ì— ì–´ëŒ‘í„° ì¶”ê°€
        layer.adapter = adapter

        # Forward hook ìˆ˜ì • (ì–´ëŒ‘í„° ì ìš©)
        original_forward = layer.forward

        def forward_with_adapter(self, *args, **kwargs):
            output = original_forward(*args, **kwargs)

            # outputì´ tupleì¸ ê²½ìš° (hidden_states, ...)
            if isinstance(output, tuple):
                hidden_states = output[0]
                hidden_states = self.adapter(hidden_states)
                return (hidden_states,) + output[1:]
            else:
                return self.adapter(output)

        layer.forward = forward_with_adapter.__get__(layer, type(layer))

    return model


def freeze_except_adapters(model: nn.Module, also_train_embeddings: bool = False):
    """
    ì–´ëŒ‘í„°ë¥¼ ì œì™¸í•œ ëª¨ë“  íŒŒë¼ë¯¸í„° í”„ë¦¬ì§•

    Args:
        model: ëª¨ë¸
        also_train_embeddings: ìž„ë² ë”©ë„ í•™ìŠµí• ì§€ ì—¬ë¶€
    """
    # ì „ì²´ freeze
    for param in model.parameters():
        param.requires_grad = False

    # ì–´ëŒ‘í„°ë§Œ unfreeze
    for name, param in model.named_parameters():
        if 'adapter' in name:
            param.requires_grad = True

    # ìž„ë² ë”©ë„ í•™ìŠµí•˜ëŠ” ê²½ìš°
    if also_train_embeddings:
        for name, param in model.named_parameters():
            if 'embed_tokens' in name or 'lm_head' in name:
                param.requires_grad = True

    # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì¹´ìš´íŠ¸
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model.parameters())

    print(f"âœ“ Trainable params: {trainable:,} / {total:,} ({trainable/total*100:.2f}%)")

    return model


def get_adapter_params(model: nn.Module) -> list:
    """ì–´ëŒ‘í„° íŒŒë¼ë¯¸í„°ë§Œ ì¶”ì¶œ"""
    return [p for n, p in model.named_parameters() if 'adapter' in n and p.requires_grad]

================
File: utils/eeve_initialization.py
================
#!/usr/bin/env python3
"""
EEVE Subword-based Initialization for Embedding Models

Qwen3-Embedding ëª¨ë¸ìš© ì´ˆê¸°í™”
- Input embedding: subtoken í‰ê· 
- Output embedding ì—†ìŒ (ìž„ë² ë”© ëª¨ë¸)
"""

import torch
import torch.nn as nn
from typing import Set
from transformers import PreTrainedTokenizer, PreTrainedModel
import logging

logger = logging.getLogger(__name__)


def initialize_new_embeddings_eeve_style(
    model: PreTrainedModel,
    old_tokenizer: PreTrainedTokenizer,
    new_tokenizer: PreTrainedTokenizer,
) -> PreTrainedModel:
    """
    EEVE ë°©ì‹ ì´ˆê¸°í™” (ìž„ë² ë”© ëª¨ë¸ìš©)

    ìƒˆ í† í° embedding = subtoken embeddingsì˜ í‰ê· 
    """
    logger.info("=" * 80)
    logger.info("EEVE-style Subword Initialization (Embedding Model)")
    logger.info("=" * 80)

    old_vocab = set(old_tokenizer.get_vocab().keys())
    new_vocab = set(new_tokenizer.get_vocab().keys())
    added_tokens = new_vocab - old_vocab

    logger.info(f"Old vocabulary: {len(old_vocab):,}")
    logger.info(f"New vocabulary: {len(new_vocab):,}")
    logger.info(f"Added tokens: {len(added_tokens):,}")

    if len(added_tokens) == 0:
        logger.warning("No new tokens to initialize!")
        return model

    # Embeddings ê°€ì ¸ì˜¤ê¸°
    embed_tokens = model.get_input_embeddings()

    if embed_tokens is None:
        logger.error("Cannot find input embeddings!")
        return model

    initialized_count = 0
    failed_count = 0

    logger.info("\nInitializing new tokens...")

    for idx, token in enumerate(added_tokens):
        try:
            new_token_id = new_tokenizer.convert_tokens_to_ids(token)

            # Old tokenizerë¡œ subword ë¶„í•´
            subtokens = old_tokenizer.tokenize(token)

            if not subtokens:
                # Fallback: random initializationì€ ì´ë¯¸ ë˜ì–´ ìžˆìŒ
                failed_count += 1
                continue

            subtoken_ids = old_tokenizer.convert_tokens_to_ids(subtokens)

            # Subtoken embedding í‰ê· 
            with torch.no_grad():
                subtoken_embeds = [embed_tokens.weight[sid] for sid in subtoken_ids]
                avg_embed = torch.stack(subtoken_embeds).mean(dim=0)
                embed_tokens.weight[new_token_id] = avg_embed

            initialized_count += 1

            if (idx + 1) % 5000 == 0:
                logger.info(f"  Progress: {idx+1}/{len(added_tokens)} tokens...")

        except Exception as e:
            failed_count += 1
            continue

    logger.info(f"\n" + "=" * 80)
    logger.info(f"Initialization Results:")
    logger.info(f"  âœ“ Successfully initialized: {initialized_count:,}")
    logger.info(f"  âœ— Failed (using random): {failed_count:,}")
    logger.info(f"  Total: {len(added_tokens):,}")
    logger.info("=" * 80 + "\n")

    return model


def get_new_token_mask(
    tokenizer: PreTrainedTokenizer,
    old_vocab_size: int
) -> torch.Tensor:
    """
    ìƒˆ í† í° ë§ˆìŠ¤í¬ ìƒì„±

    Returns:
        Boolean tensor [vocab_size], True = ìƒˆ í† í°
    """
    vocab_size = len(tokenizer)
    mask = torch.zeros(vocab_size, dtype=torch.bool)

    if vocab_size > old_vocab_size:
        mask[old_vocab_size:] = True

    return mask


def create_gradient_mask_hook(new_token_mask: torch.Tensor):
    """
    Gradient masking hook ìƒì„±

    ê¸°ì¡´ í† í°ì˜ gradientë¥¼ 0ìœ¼ë¡œ ë§Œë“¦
    """
    def hook(grad):
        """ê¸°ì¡´ í† í° gradient ì œê±°"""
        if grad is None:
            return None

        masked_grad = grad.clone()
        masked_grad[~new_token_mask] = 0.0
        return masked_grad

    return hook

================
File: utils/model_utils.py
================
"""
Model utilities for loading, expanding, and managing models
"""

import os
import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel, AutoConfig
from tqdm import tqdm

logger = logging.getLogger(__name__)


def load_tokenizers(kormo_model: str, qwen_model: str, use_fast: bool = False) -> Tuple:
    """
    KORMoì™€ Qwen í† í¬ë‚˜ì´ì € ë¡œë“œ

    Args:
        kormo_model: KORMo ëª¨ë¸ ê²½ë¡œ
        qwen_model: Qwen ëª¨ë¸ ê²½ë¡œ
        use_fast: Fast tokenizer ì‚¬ìš© ì—¬ë¶€

    Returns:
        (kormo_tokenizer, qwen_tokenizer)
    """
    logger.info("=" * 80)
    logger.info("í† í¬ë‚˜ì´ì € ë¡œë”©")
    logger.info("=" * 80)

    # KORMo
    logger.info(f"KORMo í† í¬ë‚˜ì´ì €: {kormo_model}")
    kormo_tokenizer = AutoTokenizer.from_pretrained(
        kormo_model,
        trust_remote_code=True,
        use_fast=use_fast
    )
    logger.info(f"âœ“ KORMo vocab size: {len(kormo_tokenizer):,}")

    # Qwen
    logger.info(f"Qwen í† í¬ë‚˜ì´ì €: {qwen_model}")
    qwen_tokenizer = AutoTokenizer.from_pretrained(
        qwen_model,
        trust_remote_code=True,
        use_fast=use_fast
    )
    logger.info(f"âœ“ Qwen vocab size: {len(qwen_tokenizer):,}")
    logger.info("")

    return kormo_tokenizer, qwen_tokenizer


def load_model(
    model_path: str,
    torch_dtype: str = "bfloat16",
    device_map: str = "auto"
) -> nn.Module:
    """
    ëª¨ë¸ ë¡œë“œ

    Args:
        model_path: ëª¨ë¸ ê²½ë¡œ
        torch_dtype: ë°ì´í„° íƒ€ìž…
        device_map: ë””ë°”ì´ìŠ¤ ë§µ ("auto", "cpu", "cuda", None)

    Returns:
        model
    """
    logger.info("=" * 80)
    logger.info("ëª¨ë¸ ë¡œë”©")
    logger.info("=" * 80)
    logger.info(f"ê²½ë¡œ: {model_path}")
    logger.info(f"dtype: {torch_dtype}")

    # dtype ë§¤í•‘
    dtype_map = {
        "float32": torch.float32,
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
    }
    dtype = dtype_map.get(torch_dtype, torch.bfloat16)

    # ëª¨ë¸ ë¡œë“œ (accelerate ì—†ì„ ê²½ìš° ëŒ€ë¹„)
    # configë¥¼ ëª…ì‹œí•˜ì§€ ì•Šìœ¼ë©´ checkpointì˜ ì‹¤ì œ í¬ê¸° ì‚¬ìš©
    try:
        model = AutoModel.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=dtype,
            device_map=device_map if device_map != "auto" else None
        )
    except (ValueError, ImportError):
        # accelerate ì—†ì„ ë•Œ: device_map ì—†ì´ ë¡œë“œ
        logger.warning("accelerate ì—†ìŒ - CPUë¡œ ë¡œë“œ í›„ ìˆ˜ë™ìœ¼ë¡œ GPU ì´ë™")
        model = AutoModel.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=dtype
        )
        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ì´ë™
        if torch.cuda.is_available():
            model = model.cuda()
            logger.info("âœ“ ëª¨ë¸ì„ GPUë¡œ ì´ë™")

    # use_cache ë¹„í™œì„±í™” (í•™ìŠµìš©)
    if hasattr(model.config, 'use_cache'):
        model.config.use_cache = False

    logger.info(f"âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    logger.info("")

    return model


def freeze_model_params(model: nn.Module, trainable_params: List[str]) -> int:
    """
    ëª¨ë¸ íŒŒë¼ë¯¸í„° í”„ë¦¬ì§• (trainable_paramsë§Œ í•™ìŠµ)

    Args:
        model: ëª¨ë¸
        trainable_params: í•™ìŠµí•  íŒŒë¼ë¯¸í„° ì´ë¦„ ë¦¬ìŠ¤íŠ¸

    Returns:
        í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜
    """
    logger.info("=" * 80)
    logger.info("ëª¨ë¸ íŒŒë¼ë¯¸í„° í”„ë¦¬ì§•")
    logger.info("=" * 80)

    # ì „ì²´ freeze
    for param in model.parameters():
        param.requires_grad = False

    # trainable_paramsë§Œ unfreeze
    trainable_count = 0
    for name, param in model.named_parameters():
        for trainable_name in trainable_params:
            if trainable_name in name:
                param.requires_grad = True
                trainable_count += param.numel()
                logger.info(f"âœ“ Trainable: {name} ({param.numel():,} params)")
                break

    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"\nì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
    logger.info(f"í•™ìŠµ íŒŒë¼ë¯¸í„°: {trainable_count:,} ({trainable_count/total_params*100:.2f}%)")
    logger.info("")

    return trainable_count


def expand_embeddings(
    model: nn.Module,
    qwen_tokenizer,
    kormo_tokenizer,
    vocab_diff: List[str],
    init_method: str = "mean"
) -> nn.Module:
    """
    ìž„ë² ë”© í™•ìž¥ (ì°¨ì§‘í•© ì¶”ê°€ ë°©ì‹)

    Args:
        model: Qwen ëª¨ë¸
        qwen_tokenizer: Qwen í† í¬ë‚˜ì´ì €
        kormo_tokenizer: KORMo í† í¬ë‚˜ì´ì €
        vocab_diff: ì¶”ê°€í•  í† í° ë¦¬ìŠ¤íŠ¸
        init_method: ì´ˆê¸°í™” ë°©ë²• ("mean", "random")

    Returns:
        í™•ìž¥ëœ ëª¨ë¸
    """
    logger.info("=" * 80)
    logger.info("ìž„ë² ë”© í™•ìž¥ (ì°¨ì§‘í•© ì¶”ê°€ ë°©ì‹)")
    logger.info("=" * 80)

    old_embed = model.get_input_embeddings()
    old_vocab_size = old_embed.num_embeddings
    new_vocab_size = old_vocab_size + len(vocab_diff)

    logger.info(f"ê¸°ì¡´ vocab: {old_vocab_size:,}")
    logger.info(f"ì¶”ê°€ í† í°: {len(vocab_diff):,}")
    logger.info(f"ìƒˆ vocab: {new_vocab_size:,}")
    logger.info(f"ì´ˆê¸°í™” ë°©ë²•: {init_method}")
    logger.info("")

    # ìƒˆ ìž„ë² ë”© ìƒì„±
    new_embed = nn.Embedding(
        new_vocab_size,
        old_embed.embedding_dim,
        device=old_embed.weight.device,
        dtype=old_embed.weight.dtype
    )

    # ê¸°ì¡´ ìž„ë² ë”© ë³µì‚¬
    with torch.no_grad():
        new_embed.weight[:old_vocab_size] = old_embed.weight.clone()

    # ìƒˆ í† í° ì´ˆê¸°í™”
    logger.info("ìƒˆ í† í° ìž„ë² ë”© ì´ˆê¸°í™”...")
    with torch.no_grad():
        for i, token in enumerate(tqdm(vocab_diff, desc="ì´ˆê¸°í™”")):
            new_idx = old_vocab_size + i

            if init_method == "mean":
                # Qwen í† í¬ë‚˜ì´ì €ë¡œ ë¶„í•´
                subtoken_ids = qwen_tokenizer.encode(token, add_special_tokens=False)

                if len(subtoken_ids) > 0:
                    # ì„œë¸Œí† í° ìž„ë² ë”©ì˜ í‰ê· 
                    subtoken_embeds = old_embed.weight[subtoken_ids]
                    new_embed.weight[new_idx] = subtoken_embeds.mean(dim=0)
                else:
                    # Fallback: ëžœë¤ ì´ˆê¸°í™”
                    torch.nn.init.normal_(new_embed.weight[new_idx:new_idx+1], mean=0.0, std=0.02)

            elif init_method == "random":
                torch.nn.init.normal_(new_embed.weight[new_idx:new_idx+1], mean=0.0, std=0.02)

    # ëª¨ë¸ì— ì ìš©
    model.set_input_embeddings(new_embed)

    # lm_headë„ í™•ìž¥ (ìžˆëŠ” ê²½ìš°)
    if hasattr(model, 'lm_head'):
        old_lm_head = model.lm_head
        new_lm_head = nn.Linear(
            old_lm_head.in_features,
            new_vocab_size,
            bias=old_lm_head.bias is not None,
            device=old_lm_head.weight.device,
            dtype=old_lm_head.weight.dtype
        )

        with torch.no_grad():
            new_lm_head.weight[:old_vocab_size] = old_lm_head.weight.clone()
            if old_lm_head.bias is not None:
                new_lm_head.bias[:old_vocab_size] = old_lm_head.bias.clone()

            # ìƒˆ í† í° ì´ˆê¸°í™” (ìž„ë² ë”©ê³¼ ë™ì¼í•˜ê²Œ)
            for i, token in enumerate(vocab_diff):
                new_idx = old_vocab_size + i
                subtoken_ids = qwen_tokenizer.encode(token, add_special_tokens=False)

                if len(subtoken_ids) > 0:
                    subtoken_weights = old_lm_head.weight[subtoken_ids]
                    new_lm_head.weight[new_idx] = subtoken_weights.mean(dim=0)
                else:
                    torch.nn.init.normal_(new_lm_head.weight[new_idx:new_idx+1], mean=0.0, std=0.02)

        model.lm_head = new_lm_head
        logger.info("âœ“ lm_headë„ í™•ìž¥ë¨")

    logger.info("âœ“ ìž„ë² ë”© í™•ìž¥ ì™„ë£Œ")
    logger.info("")

    return model


def save_model(model: nn.Module, tokenizer, output_dir: str):
    """
    ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ìž¥

    Args:
        model: ëª¨ë¸
        tokenizer: í† í¬ë‚˜ì´ì €
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
    """
    logger.info("=" * 80)
    logger.info("ëª¨ë¸ ì €ìž¥")
    logger.info("=" * 80)
    logger.info(f"ê²½ë¡œ: {output_dir}")

    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # í† í¬ë‚˜ì´ì € ì €ìž¥
    tokenizer.save_pretrained(output_path)
    logger.info("âœ“ í† í¬ë‚˜ì´ì € ì €ìž¥ ì™„ë£Œ")

    # ëª¨ë¸ ì €ìž¥
    model.save_pretrained(output_path)
    logger.info("âœ“ ëª¨ë¸ ì €ìž¥ ì™„ë£Œ")
    logger.info("")


def create_expanded_tokenizer(qwen_tokenizer, vocab_diff: List[str], output_dir: str):
    """
    í™•ìž¥ëœ í† í¬ë‚˜ì´ì € ìƒì„± ë° ì €ìž¥

    Args:
        qwen_tokenizer: Qwen í† í¬ë‚˜ì´ì €
        vocab_diff: ì¶”ê°€í•  í† í° ë¦¬ìŠ¤íŠ¸
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬

    Returns:
        í™•ìž¥ëœ í† í¬ë‚˜ì´ì €
    """
    logger.info("=" * 80)
    logger.info("í† í¬ë‚˜ì´ì € í™•ìž¥")
    logger.info("=" * 80)
    logger.info(f"ì¶”ê°€ í† í° ìˆ˜: {len(vocab_diff):,}")

    # í† í° ì¶”ê°€
    num_added = qwen_tokenizer.add_tokens(vocab_diff)
    logger.info(f"âœ“ {num_added:,}ê°œ í† í° ì¶”ê°€ë¨")
    logger.info(f"âœ“ ìƒˆ vocab size: {len(qwen_tokenizer):,}")

    # ì €ìž¥
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    qwen_tokenizer.save_pretrained(output_path)
    logger.info(f"âœ“ í† í¬ë‚˜ì´ì € ì €ìž¥: {output_path}")
    logger.info("")

    return qwen_tokenizer

================
File: utils/train_utils.py
================
"""
Training utilities for distributed training, optimization, and loss functions
"""

import os
import logging
from typing import Tuple, Optional

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup

logger = logging.getLogger(__name__)


def setup_distributed() -> Tuple[int, int, int]:
    """
    ë¶„ì‚° í•™ìŠµ í™˜ê²½ ì„¤ì •

    Returns:
        (rank, world_size, local_rank)
    """
    if "RANK" in os.environ and "WORLD_SIZE" in os.environ:
        rank = int(os.environ["RANK"])
        world_size = int(os.environ["WORLD_SIZE"])
        local_rank = int(os.environ.get("LOCAL_RANK", 0))
    else:
        rank = 0
        world_size = 1
        local_rank = 0

    if world_size > 1:
        dist.init_process_group(backend="nccl")
        torch.cuda.set_device(local_rank)

    return rank, world_size, local_rank


def is_main_process() -> bool:
    """ë©”ì¸ í”„ë¡œì„¸ìŠ¤ ì—¬ë¶€ í™•ì¸"""
    return not dist.is_initialized() or dist.get_rank() == 0


def barrier():
    """ë™ê¸°í™” barrier"""
    if dist.is_initialized():
        dist.barrier()


def setup_optimizer(
    model: nn.Module,
    learning_rate: float = 5e-5,
    weight_decay: float = 0.01,
    adam_beta1: float = 0.9,
    adam_beta2: float = 0.999,
    adam_epsilon: float = 1e-8
) -> AdamW:
    """
    Optimizer ì„¤ì •

    Args:
        model: ëª¨ë¸
        learning_rate: í•™ìŠµë¥ 
        weight_decay: Weight decay
        adam_beta1: Adam beta1
        adam_beta2: Adam beta2
        adam_epsilon: Adam epsilon

    Returns:
        Optimizer
    """
    # Weight decay ì œì™¸í•  íŒŒë¼ë¯¸í„°
    no_decay = ["bias", "LayerNorm.weight", "layer_norm.weight"]

    optimizer_grouped_parameters = [
        {
            "params": [p for n, p in model.named_parameters()
                      if p.requires_grad and not any(nd in n for nd in no_decay)],
            "weight_decay": weight_decay,
        },
        {
            "params": [p for n, p in model.named_parameters()
                      if p.requires_grad and any(nd in n for nd in no_decay)],
            "weight_decay": 0.0,
        },
    ]

    optimizer = AdamW(
        optimizer_grouped_parameters,
        lr=learning_rate,
        betas=(adam_beta1, adam_beta2),
        eps=adam_epsilon
    )

    return optimizer


def setup_scheduler(
    optimizer,
    num_training_steps: int,
    warmup_ratio: float = 0.1,
    scheduler_type: str = "linear"
):
    """
    Learning rate scheduler ì„¤ì •

    Args:
        optimizer: Optimizer
        num_training_steps: ì „ì²´ í•™ìŠµ ìŠ¤í… ìˆ˜
        warmup_ratio: Warmup ë¹„ìœ¨
        scheduler_type: ìŠ¤ì¼€ì¤„ëŸ¬ íƒ€ìž… ("linear", "cosine")

    Returns:
        Scheduler
    """
    num_warmup_steps = int(num_training_steps * warmup_ratio)

    if scheduler_type == "linear":
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=num_warmup_steps,
            num_training_steps=num_training_steps
        )
    elif scheduler_type == "cosine":
        scheduler = get_cosine_schedule_with_warmup(
            optimizer,
            num_warmup_steps=num_warmup_steps,
            num_training_steps=num_training_steps
        )
    else:
        raise ValueError(f"Unknown scheduler type: {scheduler_type}")

    return scheduler


def mean_pooling(
    hidden_states: torch.Tensor,
    attention_mask: torch.Tensor
) -> torch.Tensor:
    """
    Mean pooling (attention mask ê³ ë ¤)

    Args:
        hidden_states: [B, L, D]
        attention_mask: [B, L]

    Returns:
        [B, D]
    """
    # Expand mask
    mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()

    # Sum embeddings
    sum_embeddings = torch.sum(hidden_states * mask_expanded, dim=1)

    # Sum mask
    sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)

    return sum_embeddings / sum_mask


def simcse_loss(
    embeddings: torch.Tensor,
    temperature: float = 0.05
) -> torch.Tensor:
    """
    SimCSE loss (InfoNCE)
    ê°™ì€ ë¬¸ìž¥ì˜ ì„œë¡œ ë‹¤ë¥¸ dropoutì„ positive pairë¡œ ê°„ì£¼

    Args:
        embeddings: [2*B, D] - ê° ìƒ˜í”Œì´ 2ë²ˆì”© forward (ë‹¤ë¥¸ dropout)
        temperature: Temperature

    Returns:
        loss
    """
    batch_size = embeddings.size(0) // 2

    # Normalize
    embeddings = F.normalize(embeddings, p=2, dim=1)

    # Similarity matrix: [2B, 2B]
    sim_matrix = torch.matmul(embeddings, embeddings.T) / temperature

    # Positive pairs: (i, i+B) and (i+B, i)
    labels = torch.arange(batch_size, device=embeddings.device)
    labels = torch.cat([labels + batch_size, labels])  # [0+B, 1+B, ..., B-1+B, 0, 1, ..., B-1]

    # CrossEntropy loss
    loss = F.cross_entropy(sim_matrix, labels)

    return loss


def contrastive_loss(
    embeddings1: torch.Tensor,
    embeddings2: torch.Tensor,
    temperature: float = 0.05
) -> torch.Tensor:
    """
    Contrastive loss (ëŒ€ì¡° í•™ìŠµ)

    Args:
        embeddings1: [B, D]
        embeddings2: [B, D]
        temperature: Temperature

    Returns:
        loss
    """
    batch_size = embeddings1.size(0)

    # Normalize
    embeddings1 = F.normalize(embeddings1, p=2, dim=1)
    embeddings2 = F.normalize(embeddings2, p=2, dim=1)

    # Similarity matrices
    sim_11 = torch.matmul(embeddings1, embeddings1.T) / temperature
    sim_22 = torch.matmul(embeddings2, embeddings2.T) / temperature
    sim_12 = torch.matmul(embeddings1, embeddings2.T) / temperature
    sim_21 = torch.matmul(embeddings2, embeddings1.T) / temperature

    # Labels (diagonal)
    labels = torch.arange(batch_size, device=embeddings1.device)

    # Loss
    loss_12 = F.cross_entropy(sim_12, labels)
    loss_21 = F.cross_entropy(sim_21, labels)

    loss = (loss_12 + loss_21) / 2

    return loss


def save_checkpoint(
    model: nn.Module,
    optimizer,
    scheduler,
    epoch: int,
    step: int,
    loss: float,
    output_dir: str
):
    """
    ì²´í¬í¬ì¸íŠ¸ ì €ìž¥

    Args:
        model: ëª¨ë¸
        optimizer: Optimizer
        scheduler: Scheduler
        epoch: Epoch
        step: Step
        loss: Loss
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
    """
    if not is_main_process():
        return

    from pathlib import Path
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    checkpoint = {
        'epoch': epoch,
        'step': step,
        'loss': loss,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
    }

    checkpoint_path = output_path / f"checkpoint_epoch{epoch}_step{step}.pt"
    torch.save(checkpoint, checkpoint_path)

    logger.info(f"âœ“ Checkpoint saved: {checkpoint_path}")


def load_checkpoint(
    checkpoint_path: str,
    model: nn.Module,
    optimizer=None,
    scheduler=None
) -> Tuple[int, int, float]:
    """
    ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ

    Args:
        checkpoint_path: ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
        model: ëª¨ë¸
        optimizer: Optimizer (ì˜µì…˜)
        scheduler: Scheduler (ì˜µì…˜)

    Returns:
        (epoch, step, loss)
    """
    logger.info(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")

    checkpoint = torch.load(checkpoint_path, map_location='cpu')

    model.load_state_dict(checkpoint['model_state_dict'])

    if optimizer and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    if scheduler and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

    epoch = checkpoint.get('epoch', 0)
    step = checkpoint.get('step', 0)
    loss = checkpoint.get('loss', 0.0)

    logger.info(f"âœ“ ë¡œë“œ ì™„ë£Œ: Epoch {epoch}, Step {step}, Loss {loss:.4f}")

    return epoch, step, loss

================
File: base_trainer.py
================
"""
Base Trainer for Embedding Model Training
ê³µí†µ íŠ¸ë ˆì´ë„ˆ í´ëž˜ìŠ¤
"""

import os
import yaml
import logging
from pathlib import Path
from tqdm import tqdm
from datetime import timedelta

import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler

from transformers import AutoTokenizer, AutoModel, get_scheduler
from datasets import load_dataset

PROJECT_ROOT = Path(__file__).parent.parent
import sys
sys.path.insert(0, str(PROJECT_ROOT))

# Import after path setup
import importlib.util
spec = importlib.util.spec_from_file_location("contrastive_loss", PROJECT_ROOT / "utils" / "contrastive_loss.py")
contrastive_loss_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(contrastive_loss_module)
ContrastiveLoss = contrastive_loss_module.ContrastiveLoss
compute_embedding_stats = contrastive_loss_module.compute_embedding_stats


def setup_distributed():
    """DDP ì´ˆê¸°í™”"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ.get('LOCAL_RANK', 0))

        dist.init_process_group(
            backend='nccl',
            init_method='env://',
            timeout=timedelta(minutes=10)
        )

        torch.cuda.set_device(local_rank)

        if rank == 0:
            visible = os.environ.get('CUDA_VISIBLE_DEVICES', '')
            print(f"[DDP] world_size={world_size}, local_rank={local_rank}, "
                  f"CUDA_VISIBLE_DEVICES='{visible}'", flush=True)

        return rank, world_size, local_rank
    else:
        return 0, 1, 0


def cleanup_distributed():
    if dist.is_initialized():
        dist.destroy_process_group()


def is_main_process():
    return not dist.is_initialized() or dist.get_rank() == 0


class BaseEmbeddingTrainer:
    """Base Trainer for all stages"""

    def __init__(self, stage_name: str, config_path: str, model_path: str = None):
        self.stage_name = stage_name
        self.rank, self.world_size, self.local_rank = setup_distributed()
        self.device = torch.device(f'cuda:{self.local_rank}')

        # Config ë¡œë“œ
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)

        self.project_config = config['project']
        self.stage_config = config[stage_name]
        self.model_path = model_path

        self.setup_logging()
        self.prepare_model()
        self.prepare_data()
        self.prepare_optimizer()

    def setup_logging(self):
        if is_main_process():
            log_dir = PROJECT_ROOT / "logs"
            log_dir.mkdir(parents=True, exist_ok=True)
            logging.basicConfig(
                level=logging.INFO,
                format='%(asctime)s - %(levelname)s - %(message)s',
                handlers=[
                    logging.FileHandler(log_dir / f'{self.stage_name}.log'),
                    logging.StreamHandler()
                ]
            )
        self.logger = logging.getLogger(__name__)

    def log(self, message):
        if is_main_process():
            self.logger.info(message)

    def prepare_model(self):
        self.log("=" * 80)
        self.log(f"{self.stage_config['name']}")
        self.log("=" * 80)

        local_files_only = (dist.is_initialized() and dist.get_rank() != 0)

        # Load model
        if self.model_path:
            self.log(f"\nðŸ”§ Loading from: {self.model_path}")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                local_files_only=local_files_only
            )
            self.model = AutoModel.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                torch_dtype=torch.bfloat16,
                local_files_only=local_files_only
            )
        else:
            # Stage 1: load base + resize
            self.log(f"\nðŸ”§ Loading base: {self.project_config['base_model']}")
            tokenizer_path = PROJECT_ROOT / self.project_config['tokenizer_path']
            self.tokenizer = AutoTokenizer.from_pretrained(
                str(tokenizer_path),
                trust_remote_code=True
            )
            self.model = AutoModel.from_pretrained(
                self.project_config['base_model'],
                trust_remote_code=True,
                torch_dtype=torch.bfloat16,
                local_files_only=local_files_only
            )

            # Resize
            old_size = self.model.get_input_embeddings().weight.shape[0]
            new_size = len(self.tokenizer)
            self.log(f"   Resizing: {old_size:,} â†’ {new_size:,}")
            self.model.resize_token_embeddings(new_size)

        vocab_size = len(self.tokenizer)
        self.log(f"\nðŸ“š Vocabulary: {vocab_size:,}")

        if dist.is_initialized():
            dist.barrier()

        self.model = self.model.to(self.device)

        # Freeze all
        for param in self.model.parameters():
            param.requires_grad = False

        # Trainable params
        if self.stage_config.get('use_lora'):
            # LoRA
            from peft import get_peft_model, LoraConfig, TaskType

            lora_cfg = self.stage_config['lora_config']
            peft_config = LoraConfig(
                task_type=TaskType.FEATURE_EXTRACTION,
                r=lora_cfg['r'],
                lora_alpha=lora_cfg['lora_alpha'],
                lora_dropout=lora_cfg['lora_dropout'],
                target_modules=lora_cfg['target_modules'],
                bias=lora_cfg['bias']
            )
            self.model = get_peft_model(self.model, peft_config)
            self.log(f"\nâœ“ LoRA enabled (r={lora_cfg['r']})")
        else:
            # Embed tokens
            embed_tokens = self.model.get_input_embeddings()
            embed_tokens.weight.requires_grad = True

            # Gradient masking for new tokens only
            if self.stage_config.get('train_new_tokens_only'):
                old_vocab = self.stage_config['old_vocab_size']
                new_token_mask = torch.zeros(vocab_size, dtype=torch.bool, device=self.device)
                new_token_mask[old_vocab:] = True

                def gradient_mask_hook(grad):
                    if grad is None:
                        return None
                    masked_grad = grad.clone()
                    masked_grad[~new_token_mask] = 0.0
                    return masked_grad

                embed_tokens.weight.register_hook(gradient_mask_hook)
                self.log(f"\nâš ï¸  Training embed_tokens (new tokens only: {vocab_size - old_vocab:,})")
            else:
                self.log(f"\nâš ï¸  Training embed_tokens (ALL tokens: {vocab_size:,})")

        trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total = sum(p.numel() for p in self.model.parameters())

        self.log(f"\nðŸ“Š Parameters:")
        self.log(f"  Total: {total:,}")
        self.log(f"  Trainable: {trainable:,}")
        self.log(f"  Percentage: {100 * trainable / total:.2f}%")

        # DDP
        if self.world_size > 1:
            self.model = DDP(
                self.model,
                device_ids=[self.local_rank],
                output_device=self.local_rank,
                find_unused_parameters=False
            )
            self.log(f"\nâœ“ DDP (world_size={self.world_size})")

        # Contrastive Loss
        contrastive_cfg = self.stage_config['contrastive']
        self.criterion = ContrastiveLoss(
            temperature=contrastive_cfg['temperature'],
            pooling=contrastive_cfg['pooling']
        )

    def prepare_data(self):
        self.log("\n" + "=" * 80)
        self.log("Data Preparation")
        self.log("=" * 80)

        dataset_cfg = self.stage_config['dataset']
        dataset = load_dataset(
            dataset_cfg['name'],
            split='train',
            streaming=dataset_cfg.get('streaming', False)
        )

        if dataset_cfg.get('max_samples'):
            dataset = dataset.select(range(min(
                dataset_cfg['max_samples'],
                len(dataset)
            )))

        def collate_fn(examples):
            texts = [ex['text'] for ex in examples]
            encodings = self.tokenizer(
                texts,
                max_length=self.stage_config['training']['max_length'],
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )
            return {
                'input_ids': encodings['input_ids'],
                'attention_mask': encodings['attention_mask']
            }

        sampler = None
        if self.world_size > 1:
            sampler = DistributedSampler(
                dataset,
                num_replicas=self.world_size,
                rank=self.rank,
                shuffle=True
            )

        self.train_dataloader = DataLoader(
            dataset,
            batch_size=self.stage_config['training']['batch_size'],
            collate_fn=collate_fn,
            sampler=sampler,
            num_workers=4,
            pin_memory=True
        )

        self.log(f"\nâœ“ DataLoader ready")
        self.log(f"  Batch/GPU: {self.stage_config['training']['batch_size']}")
        self.log(f"  Total batches: {len(self.train_dataloader)}")

    def prepare_optimizer(self):
        self.log("\n" + "=" * 80)
        self.log("Optimizer & Scheduler")
        self.log("=" * 80)

        train_cfg = self.stage_config['training']

        self.optimizer = torch.optim.AdamW(
            filter(lambda p: p.requires_grad, self.model.parameters()),
            lr=float(train_cfg['learning_rate']),
            weight_decay=float(train_cfg['weight_decay'])
        )

        num_epochs = train_cfg['num_epochs']
        grad_accum = train_cfg['gradient_accumulation_steps']
        self.total_steps = (len(self.train_dataloader) * num_epochs) // grad_accum

        self.scheduler = get_scheduler(
            train_cfg.get('lr_scheduler_type', 'cosine'),
            optimizer=self.optimizer,
            num_warmup_steps=int(self.total_steps * train_cfg['warmup_ratio']),
            num_training_steps=self.total_steps
        )

        self.log(f"\nâœ“ AdamW (LR={train_cfg['learning_rate']})")
        self.log(f"âœ“ Scheduler: cosine")
        self.log(f"  Total steps: {self.total_steps:,}")

    def train_epoch(self, epoch: int):
        self.model.train()
        train_cfg = self.stage_config['training']
        grad_accum = train_cfg['gradient_accumulation_steps']

        if self.world_size > 1 and hasattr(self.train_dataloader.sampler, 'set_epoch'):
            self.train_dataloader.sampler.set_epoch(epoch)

        total_loss = 0
        step = 0

        if is_main_process():
            pbar = tqdm(self.train_dataloader, desc=f"Epoch {epoch}")
        else:
            pbar = self.train_dataloader

        self.optimizer.zero_grad()

        for batch_idx, batch in enumerate(pbar):
            batch = {k: v.to(self.device, non_blocking=True) for k, v in batch.items()}

            model_unwrapped = self.model.module if isinstance(self.model, DDP) else self.model
            loss, _ = self.criterion(model_unwrapped, batch['input_ids'], batch['attention_mask'])

            loss = loss / grad_accum
            loss.backward()

            if (batch_idx + 1) % grad_accum == 0:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), train_cfg['max_grad_norm'])
                self.optimizer.step()
                self.scheduler.step()
                self.optimizer.zero_grad()
                step += 1

            total_loss += loss.item() * grad_accum

            if is_main_process() and (batch_idx + 1) % self.stage_config['logging']['log_steps'] == 0:
                avg_loss = total_loss / (batch_idx + 1)
                lr = self.scheduler.get_last_lr()[0]
                pbar.set_postfix({'loss': f'{avg_loss:.4f}', 'lr': f'{lr:.2e}'})

            if step > 0 and step % self.stage_config['checkpoint']['save_steps'] == 0:
                self.save_checkpoint(epoch, step, total_loss / (batch_idx + 1))

        return total_loss / len(self.train_dataloader)

    def save_checkpoint(self, epoch: int, step: int, loss: float):
        if not is_main_process():
            return

        ckpt_dir = PROJECT_ROOT / self.stage_config['checkpoint']['output_dir'] / f"step_{step}"
        ckpt_dir.mkdir(parents=True, exist_ok=True)

        model_to_save = self.model.module if isinstance(self.model, DDP) else self.model

        if hasattr(model_to_save, 'save_pretrained'):
            model_to_save.save_pretrained(ckpt_dir)
        else:
            # LoRA model
            model_to_save.save_pretrained(ckpt_dir)

        self.tokenizer.save_pretrained(ckpt_dir)

        import json
        with open(ckpt_dir / "metadata.json", 'w') as f:
            json.dump({'epoch': epoch, 'step': step, 'loss': loss}, f, indent=2)

        self.log(f"ðŸ’¾ Checkpoint: {ckpt_dir}")

    def train(self):
        self.log("\n" + "=" * 80)
        self.log("ðŸš€ Training Start")
        self.log("=" * 80)

        num_epochs = self.stage_config['training']['num_epochs']

        for epoch in range(1, num_epochs + 1):
            self.log(f"\n{'='*80}")
            self.log(f"Epoch {epoch}/{num_epochs}")
            self.log('='*80)

            avg_loss = self.train_epoch(epoch)
            self.log(f"\nâœ“ Epoch {epoch} - Avg Loss: {avg_loss:.4f}")

            # Embedding stats
            if is_main_process() and not self.stage_config.get('use_lora'):
                model_unwrapped = self.model.module if isinstance(self.model, DDP) else self.model
                stats = compute_embedding_stats(model_unwrapped, self.tokenizer, self.device)
                self.log(f"\nðŸ“Š Embedding Stats:")
                self.log(f"  Old: {stats['old_tokens_mean_norm']:.4f} Â± {stats['old_tokens_std_norm']:.4f}")
                self.log(f"  New: {stats['new_tokens_mean_norm']:.4f} Â± {stats['new_tokens_std_norm']:.4f}")

        if is_main_process():
            final_dir = PROJECT_ROOT / self.stage_config['checkpoint']['output_dir'] / "final"
            final_dir.mkdir(parents=True, exist_ok=True)

            model_to_save = self.model.module if isinstance(self.model, DDP) else self.model

            if hasattr(model_to_save, 'save_pretrained'):
                model_to_save.save_pretrained(final_dir)
            else:
                model_to_save.save_pretrained(final_dir)

            self.tokenizer.save_pretrained(final_dir)

            self.log("\n" + "=" * 80)
            self.log(f"âœ… {self.stage_config['name']} ì™„ë£Œ!")
            self.log(f"   Output: {final_dir}")
            self.log("=" * 80)

================
File: evaluate_models.py
================
#!/usr/bin/env python3
"""
06_evaluate_models_correct.py

CORRECT evaluation: Load previous stage's base + current stage's adapters
"""

import os
import sys
from pathlib import Path
import torch
import torch.nn.functional as F
from typing import List, Dict
import numpy as np
from tqdm import tqdm

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from transformers import AutoModel, AutoTokenizer

# Import adapter utils
import importlib.util
spec = importlib.util.spec_from_file_location(
    "eeve_adapter",
    PROJECT_ROOT / "scripts/utils/eeve_adapter.py"
)
eeve_adapter = importlib.util.module_from_spec(spec)
spec.loader.exec_module(eeve_adapter)
inject_adapters = eeve_adapter.inject_adapters


# Test dataset
SIMILAR_PAIRS = [
    ("ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”.", "ë‚ ì”¨ê°€ ë§¤ìš° í™”ì°½í•©ë‹ˆë‹¤."),
    ("ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìžˆë‹¤.", "AI ê¸°ìˆ ì˜ ë°œì „ ì†ë„ê°€ ë¹ ë¥´ë‹¤."),
    ("ì €ëŠ” ì»¤í”¼ë¥¼ ë§¤ìš° ì¢‹ì•„í•©ë‹ˆë‹¤.", "ì»¤í”¼ë¥¼ ë§ˆì‹œëŠ” ê²ƒì„ ì •ë§ ì¦ê¹ë‹ˆë‹¤."),
    ("ì‚¼ì„±ì „ìžê°€ ìƒˆë¡œìš´ ìŠ¤ë§ˆíŠ¸í°ì„ ì¶œì‹œí–ˆë‹¤.", "ì‚¼ì„±ì´ ì‹ í˜• íœ´ëŒ€í°ì„ ê³µê°œí–ˆë‹¤."),
    ("ì´ ì˜í™”ëŠ” ì •ë§ ìž¬ë¯¸ìžˆì—ˆì–´ìš”.", "ì´ ì˜í™” ì§„ì§œ ìž¬ë°Œì—ˆì–´ìš”."),
    ("ì£¼ì‹ ì‹œìž¥ì´ ì˜¤ëŠ˜ í¬ê²Œ ìƒìŠ¹í–ˆë‹¤.", "ì˜¤ëŠ˜ ì¦ì‹œê°€ ê¸‰ë“±í–ˆë‹¤."),
    ("ê±´ê°•ì„ ìœ„í•´ ë§¤ì¼ ìš´ë™í•˜ê³  ìžˆì–´ìš”.", "ê±´ê°• ê´€ë¦¬ë¥¼ ìœ„í•´ ë‚ ë§ˆë‹¤ ìš´ë™ ì¤‘ì´ì—ìš”."),
    ("ì´ ìŒì‹ì€ ë„ˆë¬´ ë§µìŠµë‹ˆë‹¤.", "ì´ ìš”ë¦¬ëŠ” ë§¤ìš° ë§µë„¤ìš”."),
    ("ì„œìš¸ì˜ êµí†µ ì²´ì¦ì´ ì‹¬ê°í•˜ë‹¤.", "ì„œìš¸ ì‹œë‚´ êµí†µ ì •ì²´ê°€ ë§¤ìš° ì‹¬í•˜ë‹¤."),
    ("íŒŒì´ì¬ì€ ë°°ìš°ê¸° ì‰¬ìš´ í”„ë¡œê·¸ëž˜ë° ì–¸ì–´ë‹¤.", "íŒŒì´ì¬ì€ ì´ˆë³´ìžê°€ í•™ìŠµí•˜ê¸° ì¢‹ì€ ì–¸ì–´ë‹¤."),
]

DIFFERENT_PAIRS = [
    ("ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”.", "ë¸”ë¡ì²´ì¸ ê¸°ìˆ ì˜ ë¯¸ëž˜ëŠ” ë°ë‹¤."),
    ("ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìžˆë‹¤.", "ì €ëŠ” í”¼ìžë¥¼ ì¢‹ì•„í•©ë‹ˆë‹¤."),
    ("ì €ëŠ” ì»¤í”¼ë¥¼ ë§¤ìš° ì¢‹ì•„í•©ë‹ˆë‹¤.", "í•œêµ­ì˜ ê²½ì œ ì„±ìž¥ë¥ ì´ ë‘”í™”ë˜ê³  ìžˆë‹¤."),
    ("ì‚¼ì„±ì „ìžê°€ ìƒˆë¡œìš´ ìŠ¤ë§ˆíŠ¸í°ì„ ì¶œì‹œí–ˆë‹¤.", "ê³ ì–‘ì´ëŠ” ê·€ì—¬ìš´ ë™ë¬¼ì´ë‹¤."),
    ("ì´ ì˜í™”ëŠ” ì •ë§ ìž¬ë¯¸ìžˆì—ˆì–´ìš”.", "ìˆ˜í•™ ê³µì‹ì„ ì™¸ìš°ëŠ” ê²ƒì´ ì–´ë µë‹¤."),
    ("ì£¼ì‹ ì‹œìž¥ì´ ì˜¤ëŠ˜ í¬ê²Œ ìƒìŠ¹í–ˆë‹¤.", "ë°”ë‹¤ì—ì„œ ì„œí•‘ì„ ì¦ê¸°ê³  ìžˆì–´ìš”."),
    ("ê±´ê°•ì„ ìœ„í•´ ë§¤ì¼ ìš´ë™í•˜ê³  ìžˆì–´ìš”.", "ìžë™ì°¨ ê°€ê²©ì´ ê³„ì† ì˜¤ë¥´ê³  ìžˆë‹¤."),
    ("ì´ ìŒì‹ì€ ë„ˆë¬´ ë§µìŠµë‹ˆë‹¤.", "ì¸í„°ë„· ì†ë„ê°€ ë§¤ìš° ë¹ ë¥´ë‹¤."),
    ("ì„œìš¸ì˜ êµí†µ ì²´ì¦ì´ ì‹¬ê°í•˜ë‹¤.", "ì‹ë¬¼ì— ë¬¼ì„ ì£¼ëŠ” ê²ƒì„ ìžŠì—ˆë‹¤."),
    ("íŒŒì´ì¬ì€ ë°°ìš°ê¸° ì‰¬ìš´ í”„ë¡œê·¸ëž˜ë° ì–¸ì–´ë‹¤.", "ì—¬ë¦„ íœ´ê°€ë¥¼ ì œì£¼ë„ì—ì„œ ë³´ëƒˆë‹¤."),
]


def load_model_and_tokenizer(
    model_path: str,
    base_model_path: str = None,
    load_adapters: bool = False,
    adapter_type: str = "bottleneck"
):
    """
    CORRECT model loading:
    - For adapter stages: Load BASE from previous stage, inject adapters, load adapter weights from current stage
    """
    print(f"Loading model from: {model_path}")

    if not load_adapters:
        # Non-adapter stages (Stage 0, Stage 1)
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        model = AutoModel.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16
        )
    else:
        # Adapter stages (Stage 2, Stage 3)
        print(f"  Base model: {base_model_path}")
        print(f"  Adapter type: {adapter_type}")

        # Load BASE model from previous stage
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        model = AutoModel.from_pretrained(
            base_model_path,  # Load from PREVIOUS stage!
            trust_remote_code=True,
            torch_dtype=torch.bfloat16
        )

        # Inject adapter structure
        print(f"  Injecting {adapter_type} adapters...")
        model = inject_adapters(
            model,
            adapter_type=adapter_type,
            adapter_size=256,
            dropout=0.1,
            layer_indices=None
        )

        # Load adapter weights from CURRENT stage
        print(f"  Loading adapter weights from {model_path}...")
        from safetensors import safe_open
        state_dict = {}
        safetensors_path = Path(model_path) / "model.safetensors"

        with safe_open(str(safetensors_path), framework="pt", device="cpu") as f:
            for key in f.keys():
                if 'adapter' in key:
                    state_dict[key] = f.get_tensor(key)

        missing, unexpected = model.load_state_dict(state_dict, strict=False)
        print(f"  âœ“ Loaded {len(state_dict)} adapter parameters")

    model.eval()
    model = model.cuda()
    return model, tokenizer


def mean_pooling(hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()
    sum_embeddings = torch.sum(hidden_states * input_mask_expanded, 1)
    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    return sum_embeddings / sum_mask


def get_embeddings(texts: List[str], model, tokenizer, max_length: int = 512) -> torch.Tensor:
    inputs = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors="pt")
    inputs = {k: v.cuda() for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)
        hidden_states = outputs.hidden_states[-1]
        embeddings = mean_pooling(hidden_states, inputs['attention_mask'])
        embeddings = F.normalize(embeddings, p=2, dim=1)

    return embeddings


def compute_similarity(emb1: torch.Tensor, emb2: torch.Tensor) -> float:
    return F.cosine_similarity(emb1, emb2, dim=1).item()


def evaluate_model(model, tokenizer, model_name: str) -> Dict[str, float]:
    print(f"\n{'='*80}")
    print(f"Evaluating: {model_name}")
    print('='*80)

    similar_scores = []
    different_scores = []

    print("\n[ìœ ì‚¬í•œ ë¬¸ìž¥ ìŒ]")
    for sent1, sent2 in tqdm(SIMILAR_PAIRS, desc="Similar pairs"):
        emb1 = get_embeddings([sent1], model, tokenizer)
        emb2 = get_embeddings([sent2], model, tokenizer)
        score = compute_similarity(emb1, emb2)
        similar_scores.append(score)
        print(f"  Score: {score:.4f} | {sent1[:30]}... <-> {sent2[:30]}...")

    print("\n[ë‹¤ë¥¸ ë¬¸ìž¥ ìŒ]")
    for sent1, sent2 in tqdm(DIFFERENT_PAIRS, desc="Different pairs"):
        emb1 = get_embeddings([sent1], model, tokenizer)
        emb2 = get_embeddings([sent2], model, tokenizer)
        score = compute_similarity(emb1, emb2)
        different_scores.append(score)
        print(f"  Score: {score:.4f} | {sent1[:30]}... <-> {sent2[:30]}...")

    avg_similar = np.mean(similar_scores)
    avg_different = np.mean(different_scores)
    separation = avg_similar - avg_different

    print(f"\n{'='*80}")
    print(f"Results for {model_name}:")
    print(f"  ìœ ì‚¬ ë¬¸ìž¥ í‰ê·  ì ìˆ˜: {avg_similar:.4f} (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)")
    print(f"  ë‹¤ë¥¸ ë¬¸ìž¥ í‰ê·  ì ìˆ˜: {avg_different:.4f} (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)")
    print(f"  êµ¬ë¶„ë„ (Separation): {separation:.4f} (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)")
    print('='*80)

    return {
        'avg_similar': avg_similar,
        'avg_different': avg_different,
        'separation': separation,
        'similar_scores': similar_scores,
        'different_scores': different_scores
    }


def main():
    print("="*80)
    print("í•œêµ­ì–´ ìž„ë² ë”© ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í‰ê°€ (CORRECT)")
    print("="*80)

    # Model configurations with base_model_path for adapter stages
    models = [
        {
            "name": "Stage 0 (Vocab Expanded)",
            "path": str(PROJECT_ROOT / "outputs/koqwen-expanded"),
            "load_adapters": False,
        },
        {
            "name": "Stage 1 v2 (SimCSE 10K)",
            "path": str(PROJECT_ROOT / "checkpoints/stage1/final"),
            "load_adapters": False,
        },
        {
            "name": "Stage 2 (EEVE Adapter)",
            "path": str(PROJECT_ROOT / "checkpoints/stage2/final"),
            "base_model_path": str(PROJECT_ROOT / "checkpoints/stage1/final"),  # Load Stage 1 base!
            "load_adapters": True,
            "adapter_type": "bottleneck",
        },
        {
            "name": "Stage 3 (Hierarchical)",
            "path": str(PROJECT_ROOT / "checkpoints/stage3/final"),
            "base_model_path": str(PROJECT_ROOT / "checkpoints/stage1/final"),  # Load Stage 1 base! (Stage 3 replaces Stage 2 adapters)
            "load_adapters": True,
            "adapter_type": "hierarchical",
        },
    ]

    results = {}

    for config in models:
        if not Path(config["path"]).exists():
            print(f"\nâš ï¸  ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config['path']}")
            continue

        try:
            model, tokenizer = load_model_and_tokenizer(
                config["path"],
                base_model_path=config.get("base_model_path"),
                load_adapters=config.get("load_adapters", False),
                adapter_type=config.get("adapter_type", "bottleneck")
            )

            result = evaluate_model(model, tokenizer, config["name"])
            results[config["name"]] = result

            del model, tokenizer
            torch.cuda.empty_cache()

        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {config['name']}")
            print(f"   {str(e)}")
            import traceback
            traceback.print_exc()
            continue

    # Final comparison
    print("\n" + "="*80)
    print("ìµœì¢… ë¹„êµ ê²°ê³¼")
    print("="*80)
    print(f"{'Model':<30} {'Similarâ†‘':<12} {'Differentâ†“':<12} {'Separationâ†‘':<12}")
    print("-"*80)

    for model_name, result in results.items():
        print(f"{model_name:<30} "
              f"{result['avg_similar']:<12.4f} "
              f"{result['avg_different']:<12.4f} "
              f"{result['separation']:<12.4f}")

    print("="*80)

    if results:
        best_model = max(results.items(), key=lambda x: x[1]['separation'])
        print(f"\nðŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model[0]}")
        print(f"   êµ¬ë¶„ë„: {best_model[1]['separation']:.4f}")
        print("="*80)


if __name__ == "__main__":
    main()

================
File: stage1.py
================
#!/usr/bin/env python3
"""Stage 1: New Token Input Embeddings"""
import os
import argparse
from transformers import set_seed
from base_trainer import BaseEmbeddingTrainer, cleanup_distributed

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="configs/pipeline_config.yaml")
    parser.add_argument("--seed", type=int, default=42)
    args = parser.parse_args()

    set_seed(args.seed)
    os.environ['TOKENIZERS_PARALLELISM'] = 'false'

    try:
        trainer = BaseEmbeddingTrainer("stage1", args.config, model_path=None)
        trainer.train()
    finally:
        cleanup_distributed()

if __name__ == "__main__":
    main()

================
File: stage2.py
================
#!/usr/bin/env python3
"""Stage 2: New Token Alignment"""
import os
import argparse
from transformers import set_seed
from base_trainer import BaseEmbeddingTrainer, cleanup_distributed

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="configs/pipeline_config.yaml")
    parser.add_argument("--model_path", type=str, required=True)
    parser.add_argument("--seed", type=int, default=42)
    args = parser.parse_args()

    set_seed(args.seed)
    os.environ['TOKENIZERS_PARALLELISM'] = 'false'

    try:
        trainer = BaseEmbeddingTrainer("stage2", args.config, model_path=args.model_path)
        trainer.train()
    finally:
        cleanup_distributed()

if __name__ == "__main__":
    main()

================
File: stage3.py
================
#!/usr/bin/env python3
"""Stage 3: New Token Refinement"""
import os
import argparse
from transformers import set_seed
from base_trainer import BaseEmbeddingTrainer, cleanup_distributed

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="configs/pipeline_config.yaml")
    parser.add_argument("--model_path", type=str, required=True)
    parser.add_argument("--seed", type=int, default=42)
    args = parser.parse_args()

    set_seed(args.seed)
    os.environ['TOKENIZERS_PARALLELISM'] = 'false'

    try:
        trainer = BaseEmbeddingTrainer("stage3", args.config, model_path=args.model_path)
        trainer.train()
    finally:
        cleanup_distributed()

if __name__ == "__main__":
    main()

================
File: stage4.py
================
#!/usr/bin/env python3
"""Stage 4: Full Vocabulary Harmonization"""
import os
import argparse
from transformers import set_seed
from base_trainer import BaseEmbeddingTrainer, cleanup_distributed

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="configs/pipeline_config.yaml")
    parser.add_argument("--model_path", type=str, required=True)
    parser.add_argument("--seed", type=int, default=42)
    args = parser.parse_args()

    set_seed(args.seed)
    os.environ['TOKENIZERS_PARALLELISM'] = 'false'

    try:
        trainer = BaseEmbeddingTrainer("stage4", args.config, model_path=args.model_path)
        trainer.train()
    finally:
        cleanup_distributed()

if __name__ == "__main__":
    main()

================
File: stage5.py
================
#!/usr/bin/env python3
"""Stage 5: Transformer Enhancement with LoRA"""
import os
import argparse
from transformers import set_seed
from base_trainer import BaseEmbeddingTrainer, cleanup_distributed

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="configs/pipeline_config.yaml")
    parser.add_argument("--model_path", type=str, required=True)
    parser.add_argument("--seed", type=int, default=42)
    args = parser.parse_args()

    set_seed(args.seed)
    os.environ['TOKENIZERS_PARALLELISM'] = 'false'

    try:
        trainer = BaseEmbeddingTrainer("stage5", args.config, model_path=args.model_path)
        trainer.train()
    finally:
        cleanup_distributed()

if __name__ == "__main__":
    main()

================
File: stage6.py
================
#!/usr/bin/env python3
"""Stage 6: Advanced Contrastive Learning"""
import os
import argparse
from transformers import set_seed
from base_trainer import BaseEmbeddingTrainer, cleanup_distributed

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="configs/pipeline_config.yaml")
    parser.add_argument("--model_path", type=str, required=True)
    parser.add_argument("--seed", type=int, default=42)
    args = parser.parse_args()

    set_seed(args.seed)
    os.environ['TOKENIZERS_PARALLELISM'] = 'false'

    try:
        trainer = BaseEmbeddingTrainer("stage6", args.config, model_path=args.model_path)
        trainer.train()
    finally:
        cleanup_distributed()

if __name__ == "__main__":
    main()
