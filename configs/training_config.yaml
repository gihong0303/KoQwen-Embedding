# Training Configuration - 3단계 학습 파이프라인

# ============================================================================
# Stage 1: KOREAN-WEBTEXT - SimCSE 무지도 학습
# ============================================================================
stage1:
  name: "SimCSE_Unsupervised"
  description: "KOREAN-WEBTEXT로 새 토큰 임베딩 기초 적응"

  # Dataset
  dataset:
    name: "HAERAE-HUB/KOREAN-WEBTEXT"
    text_field: "text"
    max_samples: 10000  # 1만개 샘플 (테스트용)
    shuffle: true
    streaming: false  # 전체 다운로드

  # Training params
  training:
    num_epochs: 2
    batch_size: 256  # Safe batch for SimCSE (2x forward passes)
    gradient_accumulation_steps: 4
    max_length: 128
    learning_rate: 5e-5
    warmup_ratio: 0.1
    weight_decay: 0.01

    # SimCSE specific
    temperature: 0.05
    dropout_rate: 0.1  # Dropout for augmentation

    # Optimizer
    optimizer: "adamw"
    adam_beta1: 0.9
    adam_beta2: 0.999
    max_grad_norm: 1.0

  # Model freezing
  freeze:
    freeze_base: true  # 본체 freeze
    trainable_params:
      - "embed_tokens"  # Input embedding만 학습
      - "lm_head"       # Output embedding 학습 (옵션)

  # Checkpointing
  checkpoint:
    save_strategy: "steps"
    save_steps: 5000
    save_total_limit: 3
    output_dir: "./checkpoints/stage1"

  # Logging
  logging:
    log_steps: 1  # 매 스텝마다 로그
    eval_steps: 5000

# ============================================================================
# Stage 1 Enhanced: Quick Wins - 성능 향상 버전
# ============================================================================
stage1_enhanced:
  name: "Enhanced_SimCSE_QuickWins"
  description: "100K 샘플, 5 에폭, Curriculum Learning 적용"

  # Dataset
  dataset:
    name: "HAERAE-HUB/KOREAN-WEBTEXT"
    text_field: "text"
    max_samples: 100000  # 10K → 100K
    shuffle: true
    streaming: false

  # Training params
  training:
    num_epochs: 5  # 2 → 5
    batch_size: 2048  # 256 → 2048 (massive batch for GPU utilization)
    gradient_accumulation_steps: 4
    learning_rate: 5e-5
    warmup_ratio: 0.1
    weight_decay: 0.01

    # SimCSE specific
    temperature: 0.05
    dropout_rate: 0.1

    # Optimizer
    optimizer: "adamw"
    adam_beta1: 0.9
    adam_beta2: 0.999
    max_grad_norm: 1.0

    # Curriculum Learning
    curriculum:
      enabled: true
      phases:
        - epochs: [1, 2]
          max_length: 64
          difficulty: "easy"
        - epochs: [3, 4]
          max_length: 128
          difficulty: "medium"
        - epochs: [5]
          max_length: 256
          difficulty: "hard"

  # Model freezing
  freeze:
    freeze_base: true
    trainable_params:
      - "embed_tokens"
      - "lm_head"

  # Checkpointing
  checkpoint:
    save_strategy: "epoch"  # epoch마다 저장
    save_total_limit: 5
    output_dir: "./checkpoints/stage1_enhanced"

  # Logging
  logging:
    log_steps: 10
    eval_steps: 1000

# ============================================================================
# Stage 2: KOREAN-SyntheticText - 추가 적응
# ============================================================================
stage2:
  name: "SyntheticText_Adaptation"
  description: "생성 텍스트로 새 토큰 추가 적응"

  # Dataset
  dataset:
    name: "HAERAE-HUB/KOREAN-SyntheticText-1.5B"
    text_field: "text"
    max_samples: 500000  # 50만개 샘플
    shuffle: true
    streaming: false

  # Training params
  training:
    num_epochs: 1
    batch_size: 48
    gradient_accumulation_steps: 2
    max_length: 128
    learning_rate: 3e-5  # Stage1보다 낮게
    warmup_ratio: 0.05
    weight_decay: 0.01

    # SimCSE
    temperature: 0.05
    dropout_rate: 0.1

    # Optimizer
    optimizer: "adamw"
    adam_beta1: 0.9
    adam_beta2: 0.999
    max_grad_norm: 1.0

  # Model freezing
  freeze:
    freeze_base: true
    trainable_params:
      - "embed_tokens"
      - "lm_head"

  # Checkpointing
  checkpoint:
    save_strategy: "steps"
    save_steps: 5000
    save_total_limit: 2
    output_dir: "./checkpoints/stage2"

  # Logging
  logging:
    log_steps: 100
    eval_steps: 5000

# ============================================================================
# Stage 3: Hierarchical Adapter (Optimized)
# ============================================================================
stage3:
  name: "Hierarchical_Adapter"
  description: "Stage 1 모델에 Hierarchical Adapter 추가 (메모리 최적화)"

  # Dataset
  dataset:
    name: "HAERAE-HUB/KOREAN-WEBTEXT"
    text_field: "text"
    max_samples: 50000  # 5만개 샘플
    shuffle: true
    streaming: false

  # Training params (메모리 최적화)
  training:
    num_epochs: 3
    batch_size: 16  # OOM 방지: 24 → 16 (gradient checkpointing 없이)
    gradient_accumulation_steps: 6  # Effective batch: 16 * 6 * 6 = 576
    max_length: 128
    learning_rate: 3e-4  # Adapter는 높은 LR 가능
    warmup_ratio: 0.1
    weight_decay: 0.01

    # SimCSE
    temperature: 0.05
    dropout_rate: 0.1

    # Optimizer
    optimizer: "adamw"
    adam_beta1: 0.9
    adam_beta2: 0.999
    max_grad_norm: 1.0

  # Adapter 설정
  adapter:
    type: "hierarchical"
    size: 256
    dropout: 0.1

  # Checkpointing
  checkpoint:
    save_strategy: "steps"
    save_steps: 1000
    save_total_limit: 2
    output_dir: "./checkpoints/stage3"

  # Logging
  logging:
    log_steps: 50
    eval_steps: 1000

# ============================================================================
# Evaluation
# ============================================================================
evaluation:
  # KoSimpleEval dataset
  dataset:
    name: "HAERAE-HUB/KoSimpleEval"
    splits: ["test"]

  # Metrics
  metrics:
    - "spearman_correlation"  # STS
    - "cosine_similarity"
    - "token_efficiency"  # 토큰 개수 비교

  # Comparison
  compare_with:
    - "Qwen/Qwen3-Embedding-0.6B"  # 원본
    - "./outputs/koqwen-expanded"  # Stage 0 (확장만)
    - "./checkpoints/stage1/final"  # Stage 1
    - "./checkpoints/stage2/final"  # Stage 2
    - "./checkpoints/stage3/final"  # Stage 3 (최종)

# ============================================================================
# Common settings for all stages
# ============================================================================
common:
  # Distributed training
  distributed:
    backend: "nccl"
    find_unused_parameters: false

  # Mixed precision
  mixed_precision:
    enabled: true
    dtype: "bfloat16"

  # Data loading
  dataloader:
    num_workers: 8
    pin_memory: true
    prefetch_factor: 2

  # Reproducibility
  seed: 42
  deterministic: false  # 속도를 위해 false
