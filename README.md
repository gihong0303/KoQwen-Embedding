# ğŸ‡°ğŸ‡· Korean Embedding Expansion for Qwen3-Embedding-0.6B

**EEVE-Thunder í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•ì„ í†µí•œ í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ í™•ì¥**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)

---

## ğŸ“‹ Executive Summary

ë³¸ ì—°êµ¬ëŠ” í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ **EEVE (Efficient and Effective Vocabulary Expansion)**ì™€ **Thunder** ë°©ë²•ë¡ ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.

**Qwen3-Embedding-0.6B**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **KORMo-10B** í† í¬ë‚˜ì´ì €ì™€ì˜ ì°¨ì§‘í•© ë¶„ì„ì„ í†µí•´ **68,029ê°œì˜ í•œêµ­ì–´ íŠ¹í™” í† í°**ì„ ì¶”ê°€í•˜ì—¬ ì´ **219,698ê°œ** ì–´íœ˜ë¡œ í™•ì¥í–ˆìœ¼ë©°, **6ë‹¨ê³„ ì ì§„ì  í•™ìŠµ íŒŒì´í”„ë¼ì¸**ì„ í†µí•´ ì•ˆì •ì ì¸ ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.

---

## ğŸš€ Key Contributions

1. **ì°¨ì§‘í•© ê¸°ë°˜ ì„ íƒì  í† í° í™•ì¥ ë°©ë²•ë¡ **
   - KORMo-10Bì™€ Qwen3ì˜ ì°¨ì§‘í•© ë¶„ì„ì„ í†µí•œ í•œêµ­ì–´ íŠ¹í™” í† í° ì„ ë³„
   - Subword averaging initializationìœ¼ë¡œ ì•ˆì •ì ì¸ ì´ˆê¸°í™”

2. **EEVE-Thunder í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸**
   - EEVEì˜ ì ì§„ì  í•™ìŠµ ì „ëµ (Stage 1-4: Embeddings)
   - Thunderì˜ LoRA ê¸°ë°˜ íš¨ìœ¨ì  í•™ìŠµ (Stage 5-6: Transformers)
   - 6ë‹¨ê³„ progressive trainingìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´

3. **Contrastive Learning ê¸°ë°˜ ì„ë² ë”© ìµœì í™”**
   - SimCSE++ ìŠ¤íƒ€ì¼ì˜ ëŒ€ì¡° í•™ìŠµ
   - Causal LM loss ëŒ€ì‹  InfoNCE loss ì‚¬ìš©
   - ì„ë² ë”© ëª¨ë¸ì— íŠ¹í™”ëœ í•™ìŠµ ëª©í‘œ

4. **ì¢…í•©ì ì¸ í‰ê°€ í”„ë ˆì„ì›Œí¬**
   - 10ê°œ ë„ë©”ì¸ (ì¼ìƒëŒ€í™”, ê¸°ìˆ /IT, ê²½ì œ/ê¸ˆìœµ, ì˜ë£Œ/ê±´ê°•, ë¹„ì¦ˆë‹ˆìŠ¤, êµìœ¡, ì‚¬íšŒ/ë¬¸í™”, ë²•ë¥ /ì •ì¹˜, ìŠ¤í¬ì¸ , ê³¼í•™)
   - 69ê°œ í…ŒìŠ¤íŠ¸ ìŒìœ¼ë¡œ ë‹¤ê°ë„ í‰ê°€
   - ì¹´í…Œê³ ë¦¬ë³„ ê°œì„ ë„ ë¶„ì„

---

## ğŸ“Š 6-Stage Progressive Training Pipeline

```mermaid
graph TD
    A[Base Model: Qwen3-Embedding-0.6B<br/>Vocab: 151,669] --> B[Tokenizer Expansion<br/>+68,029 Korean tokens]
    B --> C[Stage 1-3: New Token Learning<br/>Contrastive Learning on Embeddings]
    C --> D[Stage 4: Vocabulary Harmonization<br/>Full vocabulary training]
    D --> E[Stage 5-6: LoRA Transformer Layers<br/>Reasoning & High-quality data]
    E --> F[Final Model<br/>Vocab: 219,698<br/>Separation: +24.59%]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#e1f5fe
    style D fill:#fff9c4
    style E fill:#f3e5f5
    style F fill:#c8e6c9
```

### Stage Overview

| Stage | Focus | Trainable Params | Dataset | Size | Learning Rate |
|-------|-------|------------------|---------|------|---------------|
| **Stage 1** | New token input embeddings | `embed_tokens` (new only) | KOREAN-WEBTEXT | 300K | 3e-4 |
| **Stage 2** | New token alignment | `embed_tokens` (new only) | KOREAN-WEBTEXT | 300K | 2e-4 |
| **Stage 3** | New token refinement | `embed_tokens` (new only) | KOREAN-SyntheticText | 200K | 1e-4 |
| **Stage 4** | Full vocab harmonization | `embed_tokens` (all) | Mixed (3 datasets) | 200K | 5e-5 |
| **Stage 5** | Transformer enhancement | LoRA (r=64) | Reasoning data | 200K | 5e-5 |
| **Stage 6** | Advanced contrastive | LoRA (r=32) | K2-Feedback | 150K | 3e-5 |

---

## ğŸ”¬ Methodology

### 1. Tokenizer Expansion (Difference-Based Approach)

```python
# Step 1: Vocabulary Difference Analysis
kormo_vocab = set(kormo_tokenizer.get_vocab().keys())
qwen_vocab = set(qwen_tokenizer.get_vocab().keys())

korean_specific_tokens = kormo_vocab - qwen_vocab
print(f"KORMo only tokens: {len(korean_specific_tokens)}")  # 68,029

# Step 2: Quality Filtering
filtered_tokens = [
    token for token in korean_specific_tokens
    if is_valid_korean_token(token)  # íŠ¹ìˆ˜ë¬¸ì, ì œì–´ë¬¸ì ì œê±°
]

# Step 3: Subword Averaging Initialization
for token in filtered_tokens:
    subwords = decompose_to_subwords(token)
    new_embedding = average(subwords_embeddings) + noise(0.02)
```

**Why Difference-Based?**
- âœ… í•œêµ­ì–´ íŠ¹í™” í† í°ë§Œ ì„ ë³„ (ì¤‘ë³µ ì—†ìŒ)
- âœ… ê¸°ì¡´ Qwen vocabulary ì™„ì „ ë³´ì¡´
- âœ… KORMoì˜ í•œêµ­ì–´ ìµœì í™” í† í° í™œìš©

### 2. Contrastive Learning for Embeddings

```python
class EmbeddingContrastiveLoss(nn.Module):
    """ì„ë² ë”© ëª¨ë¸ íŠ¹í™” Contrastive Loss"""

    def __init__(self, temperature=0.05, pooling='mean'):
        super().__init__()
        self.temperature = temperature
        self.pooling = pooling

    def forward(self, model, input_ids, attention_mask):
        # 1. Forward pass
        outputs = model(input_ids, attention_mask)

        # 2. Mean pooling
        embeddings = mean_pooling(outputs.last_hidden_state, attention_mask)

        # 3. Normalize
        embeddings = F.normalize(embeddings, p=2, dim=1)

        # 4. InfoNCE loss (SimCSE style)
        loss = compute_infonce_loss(embeddings, temperature)

        return loss
```

**Key Differences from Causal LM:**
- âŒ No next-token prediction
- âœ… Contrastive learning (positive/negative pairs)
- âœ… Mean pooling for sentence embeddings
- âœ… Temperature-scaled cosine similarity

### 3. Gradient Masking for New Tokens (Stage 1-3)

```python
def create_new_token_mask(model, old_vocab_size):
    """Stage 1-3: ìƒˆ í† í°ë§Œ í•™ìŠµ"""
    vocab_size = model.get_input_embeddings().weight.shape[0]
    mask = torch.zeros(vocab_size, dtype=torch.bool)
    mask[old_vocab_size:] = True  # ìƒˆ í† í°ë§Œ True

    def gradient_hook(grad):
        masked_grad = grad.clone()
        masked_grad[~mask] = 0.0  # ê¸°ì¡´ í† í° gradient ì œê±°
        return masked_grad

    model.get_input_embeddings().weight.register_hook(gradient_hook)
```

### 4. LoRA for Embedding Models (Stage 5-6)

```python
from peft import LoraConfig, TaskType, get_peft_model

# Stage 5: Higher rank LoRA (coarse adaptation)
lora_config_stage5 = LoraConfig(
    task_type=TaskType.FEATURE_EXTRACTION,  # ì„ë² ë”© ëª¨ë¸ìš©
    r=64,  # ë†’ì€ rank
    lora_alpha=128,
    lora_dropout=0.05,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    bias="none"
)

# Stage 6: Lower rank LoRA (fine refinement)
lora_config_stage6 = LoraConfig(
    r=32,  # ë‚®ì€ rank
    lora_alpha=64,
    lora_dropout=0.05,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
)
```

---

## ğŸ“š Dataset Selection

### Why HAERAE-HUB Datasets?

| Dataset | Stage | Size | Rationale |
|---------|-------|------|-----------|
| **KOREAN-WEBTEXT** | 1-2 | 300K | ëŒ€ê·œëª¨ í•œêµ­ì–´ ì›¹ í…ìŠ¤íŠ¸, ë‹¤ì–‘í•œ ë„ë©”ì¸ |
| **KOREAN-SyntheticText** | 3 | 200K | ê³ í’ˆì§ˆ í•©ì„± ë°ì´í„°, ë…¸ì´ì¦ˆ ìµœì†Œí™” |
| **Mixed (3 datasets)** | 4 | 200K | ë„ë©”ì¸ ë‹¤ì–‘ì„± í™•ë³´ |
| **HAE-RAE-COT** | 5 | 100K | ì¶”ë¡  ëŠ¥ë ¥ ê°•í™” |
| **HR-Instruct-Math** | 5 | 100K | ìˆ˜í•™ì  ì¶”ë¡  |
| **K2-Feedback** | 6 | 150K | ì¸ê°„ í”¼ë“œë°± (scoreâ‰¥5) |

### Excluded Datasets

```python
excluded_datasets = {
    "KMMLU": "í‰ê°€ ë°ì´í„°ì…‹ (í•™ìŠµ ì‹œ contamination ìœ„í—˜)",
    "HAE_RAE_BENCH": "ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° (í‰ê°€ìš©)",
    "csatqa": "ë„ˆë¬´ ì‘ì€ í¬ê¸° (1.12K)",
    "QARV-binary-set": "ì´ì§„ ë¶„ë¥˜ íƒœìŠ¤í¬ (ì„ë² ë”©ê³¼ ë¬´ê´€)"
}
```

---

## ğŸ“ Project Structure

```
ko-embedding-expansion/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ pipeline_config.yaml          # 6-stage configuration
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ tokenizer/
â”‚   â”‚   â”œâ”€â”€ 01_analyze_tokenizers.py  # Tokenizer analysis
â”‚   â”‚   â”œâ”€â”€ 02_extract_vocab_diff.py  # Difference extraction
â”‚   â”‚   â””â”€â”€ 03_expand_vocabulary.py   # Vocabulary expansion
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ base_trainer.py           # Base training class
â”‚   â”‚   â”œâ”€â”€ local_dataset_loader.py   # Dataset loader
â”‚   â”‚   â””â”€â”€ stage1.py ... stage6.py   # Stage scripts
â”‚   â””â”€â”€ comprehensive_evaluation.py   # Evaluation script
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ koqwen-expanded/              # Expanded tokenizer
â”‚   â””â”€â”€ evaluation_results/           # Evaluation results
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ stage1/final/
â”‚   â”œâ”€â”€ stage2/final/
â”‚   â”œâ”€â”€ stage3/final/
â”‚   â”œâ”€â”€ stage4/final/
â”‚   â”œâ”€â”€ stage5/final/
â”‚   â””â”€â”€ stage6/final/                 # ğŸ‰ Final model
â””â”€â”€ run_stage1.sh ... run_stage6.sh   # Training scripts
```

---

## ğŸš€ Quick Start

### Prerequisites

```bash
# Python 3.10+
# PyTorch 2.0+
# Transformers 4.36+
# PEFT (for LoRA)
pip install torch transformers peft datasets accelerate
```

### Hardware Requirements

```yaml
GPUs: 8 GPUs (A5000 24GB or equivalent)
Total VRAM: 192GB
Mixed Precision: BFloat16
Disk: ~50GB (models + checkpoints + cache)
Training Time: ~9-10 hours (total for all 6 stages)
```

### Step 1: Tokenizer Expansion

```bash
# Analyze tokenizers
python scripts/tokenizer/01_analyze_tokenizers.py

# Extract difference
python scripts/tokenizer/02_extract_vocab_diff.py

# Expand vocabulary
python scripts/tokenizer/03_expand_vocabulary.py
```

### Step 2: Run 6-Stage Training

```bash
# Stage 1: New token input embeddings
./run_stage1.sh

# Stage 2: New token alignment
./run_stage2.sh

# Stage 3: New token refinement
./run_stage3.sh

# Stage 4: Full vocabulary harmonization
./run_stage4.sh

# Stage 5: Transformer enhancement (LoRA r=64)
./run_stage5.sh

# Stage 6: Advanced contrastive learning (LoRA r=32)
./run_stage6.sh
```

### Step 3: Evaluation

```bash
# Comprehensive evaluation (10 categories, 69 test pairs)
CUDA_VISIBLE_DEVICES=0 python scripts/comprehensive_evaluation.py
```

---

## ğŸ“Š Evaluation Results

### ğŸ¯ MTEB Korean Retrieval Benchmark (Key Results)

**Evaluation Date**: November 10, 2025

6ê°œì˜ í•œêµ­ì–´ ê²€ìƒ‰ íƒœìŠ¤í¬ì—ì„œ Original (Qwen3-Embedding-0.6B)ê³¼ Stage 6 ëª¨ë¸ì„ ë¹„êµí•œ ê²°ê³¼ì…ë‹ˆë‹¤.

#### Performance Comparison (NDCG@10)

| Task | Original | Stage 6 | Change | Relative Change |
|------|----------|---------|--------|----------------|
| Ko-StrategyQA | 57.66% | 64.56% | **+6.90%** | âœ… **+12.0%** |
| MrTidyRetrieval | 27.81% | 30.21% | **+2.40%** | âœ… **+8.6%** |
| BelebeleRetrieval | 80.47% | 83.12% | **+2.66%** | âœ… **+3.3%** |
| MIRACLRetrieval | 34.88% | 35.81% | **+0.93%** | âœ… **+2.7%** |
| AutoRAGRetrieval | 74.70% | 73.10% | -1.60% | âŒ -2.1% |
| PublicHealthQA | 74.44% | 70.34% | -4.11% | âŒ -5.5% |
| **Average** | **58.33%** | **59.52%** | **+1.20%** | âœ… **+2.1%** |

#### Detailed Metrics

<details>
<summary><b>Ko-StrategyQA (âœ… Best Improvement: +12.0%)</b></summary>

| Metric | Original | Stage 6 | Change |
|--------|----------|---------|--------|
| NDCG@10 | 57.66% | 64.56% | +6.90% |
| MAP@10 | 50.84% | 58.38% | +7.54% |
| Recall@10 | 67.51% | 72.59% | +5.08% |
| Precision@10 | 12.14% | 13.19% | +1.05% |

</details>

<details>
<summary><b>MrTidyRetrieval (âœ… +8.6%)</b></summary>

| Metric | Original | Stage 6 | Change |
|--------|----------|---------|--------|
| NDCG@10 | 27.81% | 30.21% | +2.40% |
| MAP@10 | 23.01% | 24.36% | +1.35% |
| Recall@10 | 41.13% | 46.83% | +5.70% |
| Precision@10 | 4.56% | 5.15% | +0.59% |

</details>

<details>
<summary><b>BelebeleRetrieval (âœ… +3.3%)</b></summary>

| Metric | Original | Stage 6 | Change |
|--------|----------|---------|--------|
| NDCG@10 | 80.47% | 83.12% | +2.66% |
| MAP@10 | 77.53% | 80.42% | +2.89% |
| Recall@10 | 89.56% | 91.56% | +2.00% |
| Precision@10 | 8.96% | 9.16% | +0.20% |

</details>

<details>
<summary><b>MIRACLRetrieval (âœ… +2.7%)</b></summary>

| Metric | Original | Stage 6 | Change |
|--------|----------|---------|--------|
| NDCG@10 | 34.88% | 35.81% | +0.93% |
| MAP@10 | 28.33% | 29.41% | +1.08% |
| Recall@10 | 42.83% | 43.94% | +1.11% |
| Precision@10 | 8.64% | 8.50% | -0.14% |

</details>

<details>
<summary><b>AutoRAGRetrieval (âŒ -2.1%)</b></summary>

| Metric | Original | Stage 6 | Change |
|--------|----------|---------|--------|
| NDCG@10 | 74.70% | 73.10% | -1.60% |
| MAP@10 | 70.85% | 69.04% | -1.81% |
| Recall@10 | 86.84% | 85.97% | -0.87% |
| Precision@10 | 8.68% | 8.60% | -0.08% |

</details>

<details>
<summary><b>PublicHealthQA (âŒ -5.5%)</b></summary>

| Metric | Original | Stage 6 | Change |
|--------|----------|---------|--------|
| NDCG@10 | 74.44% | 70.34% | -4.11% |
| MAP@10 | 67.97% | 63.67% | -4.30% |
| Recall@10 | 94.80% | 90.91% | -3.89% |
| Precision@10 | 9.48% | 9.09% | -0.39% |

</details>

#### Key Findings

**âœ… Strengths:**
- 6ê°œ íƒœìŠ¤í¬ ì¤‘ 4ê°œì—ì„œ ì„±ëŠ¥ í–¥ìƒ (66.7% success rate)
- Ko-StrategyQAì—ì„œ ìµœëŒ€ 12.0% ê°œì„  (ì „ëµì  ì§ˆì˜ì‘ë‹µ ëŠ¥ë ¥ ê°•í™”)
- MrTidyRetrievalì—ì„œ 8.6% ê°œì„  (ëŒ€ê·œëª¨ ì½”í¼ìŠ¤ ê²€ìƒ‰ ëŠ¥ë ¥)
- ì „ì²´ í‰ê·  NDCG@10: ì ˆëŒ€ê°’ +1.20%, ìƒëŒ€ê°’ +2.1% ê°œì„ 

**âš ï¸ Areas for Improvement:**
- PublicHealthQA: -4.11% (ì˜ë£Œ ë„ë©”ì¸ íŠ¹í™” ì¿¼ë¦¬ì—ì„œ ì„±ëŠ¥ ì €í•˜)
- AutoRAGRetrieval: -1.60% (ê²½ë¯¸í•œ ì„±ëŠ¥ ì €í•˜)
- ë„ë©”ì¸ íŠ¹í™” íƒœìŠ¤í¬ì— ëŒ€í•œ ì¶”ê°€ íŒŒì¸íŠœë‹ ê³ ë ¤ í•„ìš”

**ğŸ’¡ Analysis:**
- ì¼ë°˜ì ì¸ í•œêµ­ì–´ ê²€ìƒ‰ íƒœìŠ¤í¬ì—ì„œ ê· í˜•ì¡íŒ ì„±ëŠ¥ í–¥ìƒ
- ì „ëµì /ì¶”ë¡  ê¸°ë°˜ ì¿¼ë¦¬ ì²˜ë¦¬ ëŠ¥ë ¥ í¬ê²Œ ê°œì„ 
- ì˜ë£Œ/ì „ë¬¸ ë„ë©”ì¸ì—ì„œëŠ” ì¶”ê°€ ìµœì í™” ì—¬ì§€ ì¡´ì¬
- 6ë‹¨ê³„ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì˜ íš¨ê³¼ì„± ê²€ì¦

---

### Custom Semantic Similarity Evaluation

69ê°œì˜ í•œêµ­ì–´ ë¬¸ì¥ ìŒ (10ê°œ ë„ë©”ì¸)ì„ ì‚¬ìš©í•œ ì„ë² ë”© í’ˆì§ˆ í‰ê°€ì…ë‹ˆë‹¤.

#### Overall Performance

| Metric | Original | Final (Stage 6) | Improvement |
|--------|----------|-----------------|-------------|
| **Separation Score** | 0.4342 | 0.5410 | **+24.59%** |
| Average Similar | 0.8315 | 0.7348 | -11.62% |
| Average Different | 0.3973 | 0.1939 | **+51.20%** |

#### Category-wise Improvement

| Category | Separation (Original) | Separation (Final) | Improvement |
|----------|----------------------|-------------------|-------------|
| ì¼ìƒëŒ€í™” | 0.4152 | 0.5765 | **+38.83%** |
| ìŠ¤í¬ì¸  | 0.4269 | 0.5841 | **+36.82%** |
| ê³¼í•™ | 0.4788 | 0.5986 | **+25.04%** |
| ë¹„ì¦ˆë‹ˆìŠ¤ | 0.4109 | 0.5109 | **+24.33%** |
| ê¸°ìˆ /IT | 0.4195 | 0.5186 | **+23.63%** |
| ì‚¬íšŒ/ë¬¸í™” | 0.3860 | 0.4758 | **+23.26%** |
| ì˜ë£Œ/ê±´ê°• | 0.4490 | 0.5450 | **+21.38%** |
| êµìœ¡ | 0.4563 | 0.5429 | **+18.99%** |
| ë²•ë¥ /ì •ì¹˜ | 0.4383 | 0.5102 | **+16.41%** |
| ê²½ì œ/ê¸ˆìœµ | 0.4621 | 0.5361 | **+16.00%** |

**Key Insights:**
- âœ… ëª¨ë“  ì¹´í…Œê³ ë¦¬ì—ì„œ ì¼ê´€ëœ ê°œì„  (16-39%)
- âœ… ì¼ìƒëŒ€í™”/ìŠ¤í¬ì¸  ì¹´í…Œê³ ë¦¬ì—ì„œ ê°€ì¥ í° í–¥ìƒ
- âœ… "ë‹¤ë¥¸ ë¬¸ì¥ êµ¬ë¶„" ëŠ¥ë ¥ì´ 51.20% í–¥ìƒë˜ì–´ false positive í¬ê²Œ ê°ì†Œ

---

## ğŸ”¬ Technical Details

### Token Statistics

```yaml
Original Vocabulary (Qwen3): 151,669
KORMo Vocabulary: 219,431
Difference (Korean-specific): 68,029
Final Expanded Vocabulary: 219,698
```

### Training Statistics

```yaml
Total Training Time: ~9-10 hours (8 GPUs, A5000 24GB)
GPU Hours: 72-80 hours

Trainable Parameters per Stage:
  Stage 1-3: 68,029 Ã— 1536 dims = 104M params
  Stage 4: 219,698 Ã— 1536 dims = 337M params
  Stage 5: LoRA 4 Ã— (1536 Ã— 64 Ã— 2) = 786K params
  Stage 6: LoRA 4 Ã— (1536 Ã— 32 Ã— 2) = 393K params

Total Dataset Samples: ~1.35M
```

### Embedding Quality Metrics

```python
# After Stage 6
embedding_quality = {
    "old_tokens_mean_norm": 2.40,
    "old_tokens_std_norm": 0.14,
    "new_tokens_mean_norm": 2.39,
    "new_tokens_std_norm": 0.15,
    "cross_similarity_mean": 0.18,
    "cross_similarity_std": 0.06
}
```

**Interpretation:**
- âœ… ê¸°ì¡´ í† í°ê³¼ ìƒˆ í† í°ì˜ norm ë¶„í¬ê°€ ìœ ì‚¬ (2.40 vs 2.39)
- âœ… ì•ˆì •ì ì¸ í‘œì¤€í¸ì°¨ (0.14-0.15)
- âœ… ì ì ˆí•œ êµì°¨ ìœ ì‚¬ë„ (0.18) - ë„ˆë¬´ ë†’ì§€ë„ ë‚®ì§€ë„ ì•ŠìŒ

---

## ğŸ†š Comparison with Related Work

### vs. KORMo Approach

| Aspect | KORMo | Our Approach |
|--------|-------|--------------|
| **Tokenizer Creation** | Train from scratch | Extend via difference |
| **Vocabulary Size** | 125K | 219K |
| **Training Stages** | 2-stage | 6-stage |
| **Training Objective** | Causal LM | Contrastive Learning |
| **Model Size** | 10.8B | 0.6B |
| **Focus** | Generation | Embedding |

### vs. Pure EEVE

| Enhancement | Pure EEVE | Our Approach |
|-------------|-----------|--------------|
| **Token Selection** | Random/Full | Difference-based |
| **Loss Function** | Causal LM | Contrastive |
| **Stages** | 2-3 stages | 6 stages |
| **LoRA Integration** | Optional | Stage 5-6 |

### vs. Pure Thunder

| Enhancement | Pure Thunder | Our Approach |
|-------------|--------------|--------------|
| **Model Type** | LLM (generation) | Embedding |
| **Training Focus** | Continual pretraining | Embedding optimization |
| **Stages** | 3 stages | 6 stages |
| **Objective** | Causal LM | Contrastive |

---

## ğŸ¯ Conclusions

### Key Achievements

1. **Successful Token Expansion**
   - 44.8% vocabulary increase (151,669 â†’ 219,698)
   - ì•ˆì •ì ì¸ embedding í’ˆì§ˆ ìœ ì§€

2. **Significant Performance Improvement**
   - êµ¬ë¶„ë„ 24.59% í–¥ìƒ
   - ë‹¤ë¥¸ ë¬¸ì¥ êµ¬ë¶„ ëŠ¥ë ¥ 51.20% í–¥ìƒ
   - ëª¨ë“  ì¹´í…Œê³ ë¦¬ì—ì„œ ì¼ê´€ëœ ê°œì„ 

3. **Efficient Training**
   - 6ë‹¨ê³„ progressive trainingìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
   - LoRAë¡œ parameter-efficient fine-tuning
   - ì´ 72ì‹œê°„ (6 GPUs) í•™ìŠµ ì™„ë£Œ

4. **Methodology Innovation**
   - ì°¨ì§‘í•© ê¸°ë°˜ í† í° ì„ íƒ
   - Contrastive learning for embeddings
   - EEVE-Thunder í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•

### Limitations and Future Work

1. **Scale Validation**
   - ë” í° ëª¨ë¸ (1B+)ì—ì„œì˜ ê²€ì¦ í•„ìš”
   - ë‹¤ì–‘í•œ downstream task í‰ê°€

2. **Multilingual Extension**
   - ë‹¤ë¥¸ ì–¸ì–´ë¡œì˜ í™•ì¥ ê°€ëŠ¥ì„±
   - Cross-lingual transfer learning

3. **Dataset Exploration**
   - HAERAE ë‹¤ë¥¸ ë°ì´í„°ì…‹ í™œìš©
   - Domain-specific fine-tuning

4. **Compression Analysis**
   - ì‹¤ì œ í† í°í™” íš¨ìœ¨ì„± ì¸¡ì •
   - ì¶”ë¡  ì†ë„ ê°œì„  ì •ëŸ‰í™”

---

## ğŸ™ Acknowledgments

- [**EEVE Team**](https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0) - EEVE ë°©ë²•ë¡ 
- [**Thunder Team**](https://github.com/ibm/thunder) - Thunder continual pretraining
- [**KORMo Team**](https://huggingface.co/KORMo-Team) - KORMo-10B í† í¬ë‚˜ì´ì €
- [**Qwen Team**](https://huggingface.co/Qwen) - Qwen3-Embedding base model
- [**HAERAE-HUB**](https://huggingface.co/HAERAE-HUB) - í•œêµ­ì–´ ë°ì´í„°ì…‹
- [**SimCSE**](https://github.com/princeton-nlp/SimCSE) - Contrastive learning framework

---

## ğŸ“ Citation

```bibtex
@misc{korean-embedding-expansion-2024,
  title={Korean Embedding Expansion for Qwen3-Embedding: EEVE-Thunder Hybrid Approach},
  author={gihong0303},
  year={2024},
  howpublished={\url{https://github.com/gihong0303/Test-Ko-Embedding}},
}
```

---

## ğŸ“„ License

MIT License

---

## ğŸ“§ Contact

For questions or collaboration:
- GitHub: [https://github.com/gihong0303/Test-Ko-Embedding](https://github.com/gihong0303/Test-Ko-Embedding)

---

**Project Status**: âœ… Stage 1-6 Complete | ğŸ‰ Evaluation Complete | ğŸ“Š Results Published

**Last Updated**: November 2024
